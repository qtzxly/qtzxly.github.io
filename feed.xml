<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:atom="http://www.w3.org/2005/Atom">
    <channel>
        <title>Anthony Fu</title>
        <link>https://antfu.me/</link>
        <description>Anthony Fu' Blog</description>
        <lastBuildDate>Fri, 15 Sep 2023 03:52:54 GMT</lastBuildDate>
        <docs>https://validator.w3.org/feed/docs/rss2.html</docs>
        <generator>https://github.com/jpmonette/feed</generator>
        <image>
            <title>Anthony Fu</title>
            <url>https://antfu.me/avatar.png</url>
            <link>https://antfu.me/</link>
        </image>
        <copyright>CC BY-NC-SA 4.0 2021 © Anthony Fu</copyright>
        <atom:link href="https://antfu.me/feed.xml" rel="self" type="application/rss+xml"/>
        <item>
            <title><![CDATA[Animated SVG Logo]]></title>
            <link>https://antfu.me/posts/animated-svg-logo</link>
            <guid>https://antfu.me/posts/animated-svg-logo</guid>
            <pubDate>Wed, 19 Jul 2023 00:00:00 GMT</pubDate>
            <content:encoded><![CDATA[<p>I recently replaced the logo on the top left corner with an animated SVG:</p>
<p flex>
  <a href="/favicon-animated.svg" target="_blank" important-border-none p4 ma>
    <Logo w-30 />
  </a>
</p>
<h2>Inspiration</h2>
<p>The first time I saw such stroke animations in SVG is the <a href="https://icones.js.org/collection/line-md">Material Line Icons</a> by <a href="https://github.com/cyberalien">Vjacheslav Trushkin</a>. It was cool, but I never thought about making one my own until I saw <a href="https://muan.co/">Mu-An Chiou</a>'s <a href="https://muan.co/pages/banners">banner</a> on her website. I suddenly feel like I want to be the cool guy too!</p>
<h2>Breakdown</h2>
<p>I downloaded Mu-An's SVG to read the code, cross-referencing the Material Line Icons. I found the trick is quite interesting, they animated <code>stroke-dasharray</code> to achieve the effect. This feels quite unintuitive as when you check the <a href="https://developer.mozilla.org/en-US/docs/Web/SVG/Attribute/stroke-dasharray">MDN documentation</a>, it looks like a pretty boring attribute.</p>
<p>I searched a bit more and found these two interesting articles:</p>
<ul>
<li><a href="https://jakearchibald.com/2013/animated-line-drawing-svg/">Animated line drawing in SVG</a> by Jake Archibald</li>
<li><a href="https://css-tricks.com/svg-line-animation-works/">How SVG Line Animation Works</a> by Chris Coyier</li>
</ul>
<p>They covered this technique very well, highly recommend reading them if you are interested. Basically, the animation is done by a <strong>very long</strong> dash moving, in which you will see the dash as the drawing line and the gap as empty space. The length and position of the dash are controlled by <code>stroke-dasharray</code> and <code>stroke-dashoffset</code>, which are both animatable.</p>
<h2>The Original Logo</h2>
<p>My original logo <a href="/logo.svg" target="_blank" important-border-none inline-block><img src="https://antfu.me/logo.svg" h-1.5em dark:filter-invert important-m0 inline-block alt="My Logo in SVG" /></a> comes from around 8 years ago, I draw it with a pressure-sensitive pen on my Surface Pro 4. It was used as a temporary placeholder for the portfolio I was trying to build at that time. I later image-traced it with Adobe Illustrator to get the SVG version. Surprising to recall, it has been so long since then.</p>
<div rounded shadow of-hidden border="~ base op20">
<img src="/images/animated-svg-logo-vector.png" dark:invert-95 important-m0>
</div>
<h2>Rework the Logo</h2>
<p>As we see, the animation is done by moving the dash on strokes, while my Logo is a vector outline with multiple control points. So I need to redraw it with a single stroke. It took a bit of practice to get used to the <a href="https://blog.openreplay.com/using-the-pen-tool-in-figma/">pen tool</a>, I managed to make it with Figma.</p>
<div rounded shadow of-hidden border="~ base op20">
<img src="/images/animated-svg-logo-redraw.png" dark:invert-95 important-m0>
</div>
<p>Manually adding the styles in the exported SVG,</p>
<pre><code class="language-css">@media (prefers-reduced-motion) {
  path {
    animation: none !important;
    stroke-dasharray: unset !important;
  }
}
@keyframes grow {
  0% {
    stroke-dashoffset: 1px;
    stroke-dasharray: 0 350px;
    opacity: 0;
  }
  10% {
    opacity: 1;
  }
  40% {
    stroke-dasharray: 350px 0;
  }
  /* Moving back */
  85% {
    stroke-dasharray: 350px 0;
  }
  95%, to {
    stroke-dasharray: 0 350px;
  }
}
path {
  stroke-dashoffset: 1px;
  stroke-dasharray: 350px 0;
  animation: grow 10s ease forwards infinite;
  transform-origin: center;
  stroke: #303030;
  animation-delay: 0s;
}
</code></pre>
<p>now we have a decent animated logo:</p>
<p flex>
  <a href="/favicon-animated-stroke.svg" target="_blank" important-border-none p4 ma>
    <LogoStroke w-50 />
  </a>
</p>
<p>The only downside is that the stroke is evenly thick everywhere, making it looks less like a signature. I tried to look for solutions and end up with the <a href="https://www.w3.org/Graphics/SVG/WG/wiki/Proposals/Variable_width_stroke">Variable width stroke proposal</a>, however, it does not seem to be going anywhere. Well, it's stroke anyway, it's supposed to be even. Giving the animation is super cool already, what else can I ask for?</p>
<h2>Variable Stroke Width</h2>
<p>When I almost gave up, I was playing around with Figma to do some final cleanup with the drafts, I suddenly realized that SVG does have mask support. So what if I have the original SVG shape as the mask, and let the stroke animate inside the mask? So I gave it a try and surprisingly it works!</p>
<div rounded shadow of-hidden border="~ base op20">
<img src="/images/animated-svg-logo-mask.png" dark:invert-95 important-m0>
</div>
<p>Basically we are using the mask to limit the stroke's visibility, a trick to workaround the limitation of the stroke width. Note it's not 100% pixel-perfect, as in the interaction we can't control the stroke width, so the stroke will be a bit off the mask. We can try to adjust the mask to make it look a bit better, but you will still see a big glitch when zooming in a lot. I guess it might be possible to solve this with multiple strokes and masks, but this one is already quite good to me.</p>
<p flex>
  <a href="/favicon-animated.svg" target="_blank" important-border-none p4 ma>
    <Logo w-50 />
  </a>
</p>
<p>Hope you enjoy the article, and I'd love to see your animated SVG logo too!</p>
]]></content:encoded>
            <author>hi@antfu.me (Anthony Fu)</author>
        </item>
        <item>
            <title><![CDATA[Stable Diffusion QR Code 101]]></title>
            <link>https://antfu.me/posts/ai-qrcode-101</link>
            <guid>https://antfu.me/posts/ai-qrcode-101</guid>
            <pubDate>Mon, 10 Jul 2023 05:00:00 GMT</pubDate>
            <content:encoded><![CDATA[<p>[[toc]]</p>
<p>
<span i-carbon-events mr1 /> Co-authored by <a href="https://antfu.me" target="_blank">Anthony Fu</a>, <a href="https://space.bilibili.com/339984/" target="_blank">赛博迪克朗</a>, wangcai and <a href="https://www.xiaohongshu.com/user/profile/5be8fb806b58b745447aab0f" target="_blank">代々木</a>
</p>
<blockquote>
<p><strong>This is a live document</strong>, will be updated as we learn more. Check back occasionally.</p>
</blockquote>
<p>A summary of discussions made in <a href="https://discord.gg/V9CNuqYfte">QRBTF's Discord server</a> and <a href="https://chat.antfu.me">Anthony's Discord server</a>. Thanks to everyone who participated in those servers.</p>
<h2>What's a Stable Diffusion QR Code?</h2>
<p>Images that are generated with <a href="https://stability.ai/blog/stable-diffusion-public-release">Stable Diffusion</a> with QR Codes as <a href="https://github.com/lllyasviel/ControlNet">ControlNet</a>'s input, making the QR Code data points blend into the artwork while still being scannable by QR Code readers.</p>
<figure pt-5>
  <div grid="~ cols-1 md:cols-3 gap-1" lg:scale-120 md:scale-110>
    <img src="https://antfu.me/images/ai-qrcode-101-example2.jpg" rounded shadow important-m0 />
    <img src="/images/ai-qrcode-101-example1.jpg" rounded shadow important-m0 />
    <img src="/images/ai-qrcode-101-example3.jpg" rounded shadow important-m0 />
  </div>
  <figcaption important-mt8 text-center>Examples from <a href="https://qrbtf.com/">QRBTF.com</a></figcaption>
</figure>
<p>The original idea was created by the people behind <a href="https://qrbtf.com/">QRBTF</a>, and was first revealed on <a href="https://www.reddit.com/r/StableDiffusion/comments/141hg9x/controlnet_for_qr_code/">this reddit</a> by <a href="https://www.reddit.com/user/nhciao/">nhciao</a>.</p>
<p><a href="https://qrbtf.com/">QRBTF</a> recently launched <a href="https://qrbtf.com/ai">their online generation service for open beta</a>. As of July 14th, 2023, they haven't released their model or methodology yet, you can join their <a href="https://discord.gg/V9CNuqYfte">Discord server</a> to follow the latest news.</p>
<p>The methods mentioned in this guide are <strong>based on community research and experiments</strong>.</p>
<h2>How to Generate?</h2>
<p>There are a few online services you can try, but this guide will focus on doing it locally on our own. You will need the basic knowledge of Stable Diffusion and ControlNet, a computer with a GPU (or a cloud GPU instance) to start.</p>
<p>If you are new to Stable Diffusion, we recommend reading these guides to get started:</p>
<ul>
<li><a href="https://aituts.com/stable-diffusion/">Stable Diffusion Knowledge Hub</a></li>
<li><a href="https://aituts.com/stable-diffusion-lora/">Stable Diffusion LoRA Models</a></li>
</ul>
<p>Once you set them up, there are two approaches to generating a stylized QR Code:</p>
<h3>Method A: <strong>Text to Image with ControlNet</strong></h3>
<p>Generate an image with prompts, and use ControlNet with a QR Code input to intervention the generation process.</p>
<ul>
<li><a href="/posts/ai-qrcode">Stylistic QR Code with Stable Diffusion</a> - by Anthony Fu</li>
<li><a href="/posts/ai-qrcode-refine">Refining AI Generated QR Code</a> - by Anthony Fu</li>
<li><a href="https://www.bilibili.com/video/BV1zF411R7xg/">[Video] 二维码融合技术2.0</a> - by 赛博迪克朗</li>
</ul>
<h3>Method B: <strong>Image to Image</strong></h3>
<p>Use a QR Code image as input, and let Stable Diffusion redraw each part of the QR Code. Doesn't require ControlNet.</p>
<ul>
<li><a href="https://stable-diffusion-art.com/qr-code/">How to make a QR code with Stable Diffusion</a> - by Andrew</li>
</ul>
<h3>Our Recommendation</h3>
<p>We found that Method A, <strong>Text to Image approach produces much better results</strong>, and it's easier to control the output. We will mostly focus on that approach in this guide.</p>
<h2>ControlNet Models</h2>
<p>Here are a few ControlNet models we found useful:</p>
<ul>
<li><a href="https://civitai.com/models/90940/controlnet-qr-pattern-qr-codes">QR Pattern</a></li>
<li><a href="https://huggingface.co/monster-labs/control_v1p_sd15_qrcode_monster">QR Code Monster</a></li>
<li><a href="https://huggingface.co/ioclab/ioc-controlnet/tree/main/models">IoC Lab Control Net</a>
<ul>
<li>Brightness Model</li>
<li>Illumination Model</li>
</ul>
</li>
</ul>
<p>See <a href="#model-comparison">model comparison</a> section for more details.</p>
<h2>The Code is Not Scannable</h2>
<p>Before going into details, let's picture the goal of your QR Code first. Here are 3 typical approaches listed by <a href="https://github.com/1r00t">@1r00t</a>:</p>
<ul>
<li>Artistic code that scans 100% and is obvious but creative</li>
<li>Code that is kind of hidden but with a bit of fiddling you get it</li>
<li>Code that is completely hidden as a sort of secret message</li>
</ul>
<p>All these approaches are viable. They are on a different balance between the art and the functionality. It's better to have such expectations so you can tune your models, parameters and prompts accordingly.</p>
<hr>
<h3>Scanners</h3>
<p>When the images are generated, we will use a QR Code scanner to verify if the code is scannable.</p>
<p>If your goal is to make a more blended-in QR Code, and you are okay with the code not being scannable by all QR Code readers, it's better to use an error-tolerant scanner to verify. We recommend using iOS's code <strong>scanner from the Control Center</strong>, or the scanner from <a href="https://www.wechat.com/en/">WeChat</a> to verify your QR Code. They are the most tolerant ones we found so far.</p>
<p>Meanwhile, if you failed to find a good scanner on your phone, or want to verify the QR Codes directly in your computer, we recently enrolled a <a href="https://qrcode.antfu.me/#scan">new scanner in Anthony's QR Toolkit</a>, based on <a href="https://docs.opencv.org/4.5.4/d5/d04/classcv_1_1wechat__qrcode_1_1WeChatQRCode.html">WeChat's open sourced algorithm</a> (Ported to WebAssembly, source code at <a href="https://github.com/antfu/qr-scanner-wechat">antfu/qr-scanner-wechat</a>).</p>
<p><img src="/images/ai-qrcode-101-toolkit-scanner.png" alt=""></p>
<hr>
<h3>Compare with the Original QR Code</h3>
<p>You can use <a href="https://qrcode.antfu.me/">Anthony's QR Toolkit</a> to compare the generated QR Code with the original one. It will show you the mismatches and help you to optimize the generation process.</p>
<p><img src="/images/ai-qrcode-refine-compare-2.png" alt=""></p>
<p>Read more about it in <a href="/posts/ai-qrcode-refine">this post</a>.</p>
<hr>
<h2>Parameters</h2>
<h3>ControlNet</h3>
<p>The parameters of the ControlNet affect when and how the control is applied to the generation process.</p>
<ul>
<li><strong>Control weight</strong> - The weight of the ControlNet. The higher the weight, the more the output will be affected by the ControlNet.</li>
<li><strong>Start control step</strong> - The percentage of the generation process when the ControlNet starts to take effect.</li>
<li><strong>End control step</strong> - The percentage of the generation process when the ControlNet stops taking effect.</li>
</ul>
<div relative flex="~ col items-center" py3>
  <div absolute top-0 left="1/14" translate-x="-1/2" hidden md:block>prompts</div>
  <div absolute top-0 left="5/12" translate-x="-1/2">prompts + control net</div>
  <div absolute top-0 left="10/12" translate-x="-1/2">prompts</div>
  <div w-full mt-1.5em h-1em rounded bg-gray:10 border="~ base" relative>
    <div absolute left="1/6" bg-yellow op80 w="3/6" h-full/>
  </div>
  <div absolute top-2.7em flex="~ col items-center" left="1/6" translate-x="-1/2">
    <div i-ri-arrow-up-s-fill text-lg/>
    <div>Control Start</div>
  </div>
   <div absolute top-2.7em flex="~ col items-center" left="4/6" translate-x="-1/2">
    <div i-ri-arrow-up-s-fill text-lg/>
    <div>Control End</div>
  </div>
</div>
<div mt-14 />
<p>The start control step will allow the prompts and the model to be creative before it knows the QR Code control exists. And the end control step will allow the model to try to blend the QR Code into the artwork more (but will make the code less scannable).</p>
<p>It requires a few trials and errors to find the right balance so that the ControlNet has enough time to intervene, but not too much so the code can be artistic enough.</p>
<p>Different models might have different strengths of the control, so you might need to adjust the parameters accordingly. It's better to read their instructions first.</p>
<hr>
<div border="~ rounded-full base" px3 py1 inline text-sm float-right>
<span i-ri-book-2-line /> Credits to <a href="https://space.bilibili.com/339984/" target="_blank">赛博迪克朗</a>
</div>
<h3>Model Comparison</h3>
<p>Thanks a lot to <a href="https://space.bilibili.com/339984/">赛博迪克朗</a> for running the following matrixes against each model.</p>
<p>Here is the original image (without ControlNet) and the QR Code Input:</p>
<div grid="~ cols-2 gap-4">
  <figure important-m0>
    <img src="/images/ai-qrcode-101-multi-cn-original.png" rounded shadow  />
    <figcaption text-center>
      Original Image
    </figcaption>
  </figure>
  <figure important-m0>
    <img src="/images/ai-qrcode-101-multi-cn-qr.png" rounded shadow  />
    <figcaption text-center>
      QR Code
    </figcaption>
  </figure>
</div>
<p>The comparison matrixes are generated with the exact same prompts and same seed as the original image, but only the parameters of the ControlNet are changed.</p>
<details>
<summary cursor-pointer select-none>Detailed prompts and paramaters</summary>
<div class="code-wrap" border="~ base rounded" px4 pt3 mt2>
<pre><code class="language-ruby">best quality, masterpiece, depth of field, 1girl, dress, trees, flowers, sky, water
</code></pre>
<pre><code class="language-ruby">NSFW, nude, bad-hands-5, bad-picture-chill-75v, badhandv4, easynegative, ng_deepnegative_v1_75t, verybadimagenegative_v1.3, bhands-neg, watermark, character watermark, photo date watermark, Date watermarking
</code></pre>
<ul>
<li>Checkpoint: <a href="https://civitai.com/models/28779/primemix">PrimeMix</a></li>
<li>Steps: 50</li>
<li>Sampler: DPM++ 2M SDE Karras</li>
<li>CFG scale: 7</li>
<li>Seed: 1902542336</li>
<li>Size: 768x1024</li>
</ul>
</div>
</details>
<p>You can <strong>drag the sliders</strong> below to see the difference between the start and end control steps:</p>
<h4><a href="https://civitai.com/models/90940/controlnet-qr-pattern-qr-codes">QR Pattern Model</a></h4>
<div grid="~ cols-1 md:cols-2 gap-2">
  <QRCodeMatrix
    src="/images/ai-qrcode-101-matrix-pattern-start.webp"
    xTitle="Weight"
    :xScale="{ min: 0.7, max: 1.6, step: 0.1}"
    :xValue="3"
    yTitle="Start"
    :yScale="{ min: 0.1, max: 0.5, step: 0.1}"
    :aspectRatio="0.75"
    :fixedRowsAfter="[['End', '1.0']]"
    :fixedRowsBefore="[['Model', 'QR Pattern']]"
  >
    <template #post="{ xValue, yValue }">
      <QRCodeControlNetScale :start="+yValue" :end="1" :weight="+xValue" />
    </template>
  </QRCodeMatrix>
  <QRCodeMatrix
    src="/images/ai-qrcode-101-matrix-pattern-end.webp"
    xTitle="Weight"
    :xScale="{ min: 0.7, max: 1.6, step: 0.1 }"
    :xValue="3"
    yTitle="End"
    :yScale="{ min: 0.4, max: 1.0, step: 0.1 }"
    :aspectRatio="0.75"
    :fixedRowsBetween="[['Start', '0']]"
    :fixedRowsBefore="[['Model', 'QR Pattern']]"
  >
    <template #post="{ xValue, yValue }">
      <QRCodeControlNetScale :start="0" :end="+yValue" :weight="+xValue" />
    </template>
  </QRCodeMatrix>
</div>
<h4><a href="https://huggingface.co/monster-labs/control_v1p_sd15_qrcode_monster">QR Code Monster Model</a></h4>
<div grid="~ cols-1 md:cols-2 gap-2">
  <QRCodeMatrix
    src="/images/ai-qrcode-101-matrix-monster-start.webp"
    xTitle="Weight"
    :xScale="{ min: 0.7, max: 1.6, step: 0.1 }"
    :xValue="3"
    yTitle="Start"
    :yScale="{ min: 0.1, max: 0.5, step: 0.1 }"
    :aspectRatio="0.75"
    :fixedRowsAfter="[['End', '1.0']]"
    :fixedRowsBefore="[['Model', 'QR Code Monster']]"
  >
    <template #post="{ xValue, yValue }">
      <QRCodeControlNetScale :start="+yValue" :end="1" :weight="+xValue" />
    </template>
  </QRCodeMatrix>
  <QRCodeMatrix
    src="/images/ai-qrcode-101-matrix-monster-end.webp"
    xTitle="Weight"
    :xScale="{ min: 0.7, max: 1.6, step: 0.1 }"
    :xValue="3"
    yTitle="End"
    :yScale="{ min: 0.4, max: 1.0, step: 0.1 }"
    :aspectRatio="0.75"
    :fixedRowsBetween="[['Start', '0']]"
    :fixedRowsBefore="[['Model', 'QR Code Monster']]"
  >
    <template #post="{ xValue, yValue }">
      <QRCodeControlNetScale :start="0" :end="+yValue" :weight="+xValue" />
    </template>
  </QRCodeMatrix>
</div>
<h4><a href="https://huggingface.co/ioclab/ioc-controlnet/tree/main/models">IoC Lab Brightness Model</a></h4>
<div grid="~ cols-1 md:cols-2 gap-2">
  <QRCodeMatrix
    src="/images/ai-qrcode-101-matrix-brightness-start.webp"
    xTitle="Weight"
    :xScale="{ min: 0.1, max: 0.9, step: 0.1 }"
    :xValue="1"
    yTitle="Start"
    :yScale="{ min: 0, max: 0.5, step: 0.1 }"
    :aspectRatio="0.75"
    :fixedRowsAfter="[['End', '0']]"
    :fixedRowsBefore="[['Model', 'IoC Lab Brightness']]"
  >
    <template #post="{ xValue, yValue }">
      <QRCodeControlNetScale :start="+yValue" :end="1" :weight="+xValue" />
    </template>
  </QRCodeMatrix>
  <QRCodeMatrix
    src="/images/ai-qrcode-101-matrix-brightness-end.webp"
    xTitle="Weight"
    :xScale="{ min: 0.1, max: 0.9, step: 0.1 }"
    :xValue="2"
    yTitle="End"
    :yScale="{ min: 0.5, max: 1.0, step: 0.1 }"
    :aspectRatio="0.75"
    :fixedRowsBetween="[['Start', '0']]"
    :fixedRowsBefore="[['Model', 'IoC Lab Brightness']]"
  >
    <template #post="{ xValue, yValue }">
      <QRCodeControlNetScale :start="0" :end="+yValue" :weight="+xValue" />
    </template>
  </QRCodeMatrix>
</div>
<hr>
<h2>Improve the Result</h2>
<p>Say that you already generated a bunch of QR Codes and find some of them you like. You want to improve them to make them more scannable, or more blended-in, or more artistic. Here are some tips we found useful.</p>
<h3>Tweak the Input</h3>
<p>The <strong>input QR Code is one of the most important parts</strong> of the whole process to generate well-blended code.</p>
<p>You can refer to <a href="/posts/ai-qrcode-refine#generating-the-base-qr-code">this post</a> to see a comparison of how different QR Code input affects the output.</p>
<p><img src="/images/ai-qrcode-refine-input-compare.jpg" alt="Comparison grid between different styled QR Code as input"></p>
<p>We recommend using <a href="https://qrcode.antfu.me/">Anthony's QR Toolkit</a> to generate the QR Code. It allows you to customize the QR Code and distort it as needed.</p>
<p>Meanwhile, the margin area of the QR Code also affects the look and feel, for example:</p>
<div flex="~ col items-center gap-4" py4>
<QRCodeCompare scale-85 md:scale-100 h-100 input="/images/ai-qrcode-101-input-edit1-i.png" output="/images/ai-qrcode-101-input-edit1-o.jpg" />
<div><div i-ri-arrow-down-line/> Adding some noise to the margin</div>
<QRCodeCompare scale-85 md:scale-100 h-100 input="/images/ai-qrcode-101-input-edit2-i.png" output="/images/ai-qrcode-101-input-edit2-o.jpg" />
<div><div i-ri-arrow-down-line/> Manually connect some points in margin (Photoshop etc.)</div>
<QRCodeCompare scale-85 md:scale-100 h-100 input="/images/ai-qrcode-101-input-edit6-i.png" output="/images/ai-qrcode-101-input-edit6-o.jpg" />
</div>
<hr>
<div border="~ rounded-full base" px3 py1 inline text-sm float-right>
<span i-ri-book-2-line /> Credits to <a href="https://www.xiaohongshu.com/user/profile/5be8fb806b58b745447aab0f" target="_blank">代々木</a>
</div>
<h3>Improve the Prompts</h3>
<p>Theoretically, you can use any prompts to generate those QR Codes.</p>
<p>To help the QR codes more blend in, we find that it's helpful to include some fluidity or fragmented items in the prompts.</p>
<h4>Example Outputs</h4>
<div py15>
  <div grid="~ md:cols-3 cols-2 gap-x-2 gap-y-4" lg:scale-110 md:scale-105>
    <figure important-my-0>
      <img src="/images/ai-qrcode-101-prompt-ribbon.jpg" rounded-md shadow />
      <figcaption text-center>
        <b text-lg>ribbon</b>
        <div text-xs mt1>by <a href="https://www.xiaohongshu.com/user/profile/5be8fb806b58b745447aab0f" target="_blank">代々木</a></div>
      </figcaption>
    </figure>
    <figure important-my-0>
      <img src="/images/ai-qrcode-101-prompt-feather.jpg" rounded-md shadow />
      <figcaption text-center>
        <b text-lg>feather</b>
        <div text-xs mt1>by <a href="https://www.xiaohongshu.com/user/profile/5be8fb806b58b745447aab0f" target="_blank">代々木</a></div>
      </figcaption>
    </figure>
    <figure important-my-0>
      <img src="/images/ai-qrcode-refine-distort-result.png" rounded-md shadow />
      <figcaption text-center>
        <b text-lg>plants</b>
        <div text-xs mt1>by <a href="https://antfu.me" target="_blank">Anthony Fu</a></div>
      </figcaption>
    </figure>
    <figure important-my-0>
      <img src="/images/ai-qrcode-101-prompt-bird.jpg" rounded-md shadow />
      <figcaption text-center>
        <b text-lg>bird</b>
        <div text-xs mt1>by <a href="https://www.xiaohongshu.com/user/profile/5be8fb806b58b745447aab0f" target="_blank">代々木</a></div>
      </figcaption>
    </figure>
    <figure important-my-0>
      <img src="/images/ai-qrcode-101-prompt-lace.jpg" rounded-md shadow />
      <figcaption text-center>
        <b text-lg>lace</b>
        <div text-xs mt1>by <a href="https://antfu.me" target="_blank">Anthony Fu</a></div>
      </figcaption>
    </figure>
    <figure important-my-0>
      <img src="/images/ai-qrcode-101-prompt-snow.jpg" rounded-md shadow />
      <figcaption text-center>
        <b text-lg>snow</b>
        <div text-xs mt1>by <a href="https://antfu.me" target="_blank">Anthony Fu</a></div>
      </figcaption>
    </figure>
    <figure important-my-0>
      <img src="/images/ai-qrcode-101-prompt-wave.png" rounded-md shadow />
      <figcaption text-center>
        <b text-lg>wave</b>
        <div text-xs mt1>by <a href="https://v.douyin.com/iDLHquJ/" target="_blank">五倍速企鹅</a></div>
      </figcaption>
    </figure>
     <figure important-my-0>
      <img src="/images/ai-qrcode-101-prompt-katana-swords.jpg" rounded-md shadow />
      <figcaption text-center>
        <b text-lg>katana swords</b>
        <div text-xs mt1>by <a href="https://www.xiaohongshu.com/user/profile/5be8fb806b58b745447aab0f" target="_blank">代々木</a></div>
      </figcaption>
    </figure>
    <figure important-my-0>
      <img src="/images/ai-qrcode-101-prompt-shibori-patterns.jpg" rounded-md shadow />
      <figcaption text-center>
        <b text-lg>shibori patterns</b>
        <div text-xs mt1>by <a href="https://www.xiaohongshu.com/user/profile/5be8fb806b58b745447aab0f" target="_blank">代々木</a></div>
      </figcaption>
    </figure>
    <figure important-my-0>
      <img src="/images/ai-qrcode-101-prompt-buildings.png" rounded-md shadow />
      <figcaption text-center>
        <b text-lg>buildings</b>
        <div text-xs mt1>by <a href="https://space.bilibili.com/251938958" target="_blank">阿帝</a></div>
      </figcaption>
    </figure>
    <figure important-my-0>
      <img src="/images/ai-qrcode-101-prompt-leaf.png" rounded-md shadow />
      <figcaption text-center>
        <b text-lg>leaf</b>
        <div text-xs mt1>by <a href="https://v.douyin.com/iDLHquJ/" target="_blank">五倍速企鹅</a></div>
      </figcaption>
    </figure>
  </div>
</div>
<h4>Example Prompts</h4>
<p class="code-wrap">
<p><strong>Ribbon</strong> - by <a href="https://www.xiaohongshu.com/user/profile/5be8fb806b58b745447aab0f">代々木</a></p>
<pre><code class="language-ruby">(1 girl:1.6), full body, from side, ultra wide shot, (azure blue dress:1.3), (grey long hair:1.3), (white ribbon:1.6), (white lace:1.6), BREAK, (dark background:1.3)
</code></pre>
<p><strong>Feather</strong> - by <a href="https://www.xiaohongshu.com/user/profile/5be8fb806b58b745447aab0f">代々木</a></p>
<pre><code class="language-ruby">(1 girl:1.3), upper body, (grey long hair:1.3), (blue dress:1.3), zigzag patterns, graphic impact, (white feathers:1.6), BREAK, (dark background:1.3)
</code></pre>
<p><strong>Birds</strong> - by <a href="https://www.xiaohongshu.com/user/profile/5be8fb806b58b745447aab0f">代々木</a></p>
<pre><code class="language-ruby">(1 girl:1.3), upper body, rosemaling patterns, Norwegian folk art, decorative designs, vibrant colors, (white birds:1.6), BREAK, (dark background:1.3)
</code></pre>
<p><strong>Wave</strong> - by <a href="https://v.douyin.com/iDLHquJ/">五倍速企鹅</a></p>
<pre><code class="language-ruby">(1 girl:1.3),(white dress:1.3), upper body, blonde hair, from side, decorative designs, (wave:1.3),BREAK, (blue background:1.3)
</code></pre>
<p><strong>Leaf</strong> - by <a href="https://v.douyin.com/iDLHquJ/">五倍速企鹅</a></p>
<pre><code class="language-ruby">(1 girl:1.3),(pink dress:1.3), upper body, white hair, from side, decorative designs, (leaf:1.6),BREAK, (sunset background:1.3)
</code></pre>
</p>
<hr>
<div border="~ rounded-full base" px3 py1 inline text-sm float-right>
<span i-ri-book-2-line /> Credits to wangcai
</div>
<h3>XYZ Plot</h3>
<p>In case you are uncertain about which model or prompts to use, you can utilize the XYZ Plot script to generate a matrix of images with different prompts and models for easier comparison.</p>
<p><img src="/images/ai-qrcode-101-xyz.png" alt=""></p>
<p>You can learn more about how to use XYZ plot in <a href="https://gigazine.net/gsc_news/en/20220909-automatic1111-stable-diffusion-webui-prompt-matrix/">this tutorial</a>.</p>
<p>Below is an example of a matrix runned by <code>wangcai</code>, testing some popular checkpoint models and the prompts we mentioned above. You can click to select different combinations, or click the image to see the full matrix.</p>
<QRCodeMatrixModelPrompts />
<p>Similarly, this is a matrix testing samplers:</p>
<QRCodeMatrixModelSamplers />
<p>We encourage you to try different prompts and models to find the best combination for your use case.</p>
<hr>
<div border="~ rounded-full base" px3 py1 inline text-sm float-right>
<span i-ri-book-2-line /> Credits to <a href="https://space.bilibili.com/339984/" target="_blank">赛博迪克朗</a>
</div>
<h3>Non-Square Image</h3>
<p>To make the QR Code less obvious, you can try to generate a non-square image, leaving some extra space around the QR Code for the Stable Diffusion to be creative. With that, you can shift the focus of the viewers to the other parts of the image.</p>
<figure>
  <img src="/images/ai-qrcode-101-non-square-example3.jpg" rounded shadow />
  <figcaption text-center>
    by <a href="https://antfu.me/" target="_blank">Anthony Fu</a>
  </figcaption>
</figure>
<figure>
  <img src="/images/ai-qrcode-101-non-square-example1.jpg" rounded shadow />
  <figcaption text-center>
    by <a href="https://antfu.me/" target="_blank">Anthony Fu</a>
  </figcaption>
</figure>
<p>To generate a non-square image, you can change the <strong>Resize Mode</strong> in ControlNet to <code>Resize and Fill</code> and change the Text to Image width or height.</p>
<p><img src="/images/ai-qrcode-101-non-square-resize.png" alt=""></p>
<p>Or in the <a href="https://qrcode.antfu.me/">Toolkit</a>, you click the <span i-carbon-chevron-down/> button on <strong>Margin</strong> to expand the option and have different margins for each side.</p>
<p><img src="/images/ai-qrcode-101-non-square-toolkit.png" alt=""></p>
<hr>
<div border="~ rounded-full base" px3 py1 inline text-sm float-right>
<span i-ri-book-2-line /> Credits to <a href="https://www.instagram.com/terryberrystudio" target="_blank">lameguy</a>
</div>
<h3>Perspective</h3>
<p>You can also try to apply some perspective transformation to the QR Code to make it more interesting.</p>
<div grid="~ cols-2 gap-2">
  <figure>
    <img src="/images/ai-qrcode-101-perspective-ep1.png" rounded shadow />
    <figcaption text-center>
      by <a href="https://www.instagram.com/terryberrystudio" target="_blank">lameguy</a>
    </figcaption>
  </figure>
  <figure>
    <img src="/images/ai-qrcode-101-perspective-ep2.png" rounded shadow />
    <figcaption text-center>
      by <a href="https://www.instagram.com/terryberrystudio" target="_blank">lameguy</a>
    </figcaption>
  </figure>
</div>
<hr>
<div border="~ rounded-full base" px3 py1 inline text-sm float-right>
<span i-ri-book-2-line /> Credits to <a href="https://space.bilibili.com/339984/" target="_blank">赛博迪克朗</a>
</div>
<h3>Multiple ControlNet</h3>
<p>Multiple ControlNet layers are mainly used to increase the recognizability of the image when the model control is insufficient. Try to avoid the result deviation caused by excessive changes in the picture, causing the ideal picture cannot be obtained.</p>
<p>Difficulties in recognition may be due to changes in prompts or due to the characteristics of the SD model, resulting in too trivial details of the picture or too bright/dark overall tone to make it impossible to recognize.</p>
<p>This method can effectively improve the automatic recognition success rate of scanning.</p>
<p>Usually, we use <strong>QR Code Monster</strong> or <strong>QR Code Pattern</strong> model as the main guidance model, and use the <strong>Brightness Model</strong> from IoC Lab as the auxiliary model to improve the local contrast.</p>
<blockquote>
<p><span i-ri-lightbulb-line text-yellow/> 赛博迪克朗: It's recommended to use the QR Monster model. The QR Pattern v2.0 still has too much interference, which may cause a great change in the style of the image.</p>
</blockquote>
<p>For example, running the same prompts as <a href="#model-comparison">the previous example</a>, when using the <strong>QR Code Monster</strong> model alone (single model), with control steps 0.0 to 1.0, we got the following results with different weights:</p>
<div grid="~ cols-2 md:cols-3 gap-2">
  <figure important-mb0 important-mt-2>
    <img src="/images/ai-qrcode-101-multi-cn-monster-w100.png" rounded shadow  />
    <figcaption text-center>
      Weight: 1.0
    </figcaption>
  </figure>
  <figure important-mb0 important-mt-2>
    <img src="/images/ai-qrcode-101-multi-cn-monster-w125.png" rounded shadow  />
    <figcaption text-center>
      Weight: 1.25
    </figcaption>
  </figure>
  <figure important-mb0 important-mt-2>
    <img src="/images/ai-qrcode-101-multi-cn-monster-w140.png" rounded shadow  />
    <figcaption text-center>
      Weight: 1.4
    </figcaption>
  </figure>
  <figure  important-mb0 important-mt-2>
    <img src="/images/ai-qrcode-101-multi-cn-monster-w150.png" rounded shadow  />
    <figcaption text-center>
      Weight: 1.5
    </figcaption>
  </figure>
  <figure important-mb0 important-mt-2>
    <img src="/images/ai-qrcode-101-multi-cn-monster-w160.png" rounded shadow  />
    <figcaption text-center>
      Weight: 1.6
    </figcaption>
  </figure>
  <figure important-mb0 important-mt-2>
    <img src="/images/ai-qrcode-101-multi-cn-monster-w170.png" rounded shadow  />
    <figcaption text-center>
      Weight: 1.7
    </figcaption>
  </figure>
</div>
<p>We notice that only Weight 1.5 and 1.7 are scannable (and do not have very good error tolerant), and we also see the compositions of them are changed a lot as the weight increases.</p>
<p>So if we want to keep the original composition but still have good enough recognizability, we could add the <strong>Brightness Model</strong> as the second model.</p>
<div grid="~ cols-1 md:cols-2 gap-4">
 <figure important-m0>
    <img src="/images/ai-qrcode-101-multi-cn-monster-w100-s00-e10-brightness-w015-s01-e10.png" rounded shadow  />
    <figcaption text-center font-mono important-text-xs>
      Monster &nbsp;&nbsp;: Weight <b>1.00</b> Start <b>0.0</b> End <b>1.0</b><br>
      Brightness: Weight <b>0.15</b> Start <b>0.1</b> End <b>1.0</b>
    </figcaption>
  </figure>
  <figure important-m0>
    <img src="/images/ai-qrcode-101-multi-cn-monster-w100-s00-e10-brightness-w025-s04-e08.png" rounded shadow  />
    <figcaption text-center font-mono important-text-xs>
      Monster &nbsp;&nbsp;: Weight <b>1.00</b> Start <b>0.0</b> End <b>1.0</b><br>
      Brightness: Weight <b>0.25</b> Start <b>0.4</b> End <b>0.8</b>
    </figcaption>
  </figure>
</div>
<p>We can see that even if we reduce the weight of the <strong>Monster Model</strong> to 1.0, the recognizability is as good as the single model with the Weight 1.5, while the composition is closer to the original image.</p>
<p>If you want to go further, it's also possible to try more models. For example, here is the result of using <strong>QR Code Monster</strong> and <strong>Brightness Model</strong> together with <strong>QR Pattern</strong>:</p>
<div grid="~ cols-1 md:cols-[1fr_2fr_1fr] gap-4 justify-center">
  <div />
  <figure important-m0>
    <img src="/images/ai-qrcode-101-multi-cn-monster-monster-w100-brightness-w010-s04-e08-pattern-w010-s04-e08.png" rounded shadow  />
    <figcaption text-center font-mono important-text-xs>
      Monster &nbsp;&nbsp;: Weight <b>1.00</b> Start <b>0.0</b> End <b>1.0</b><br>
      Brightness: Weight <b>0.10</b> Start <b>0.4</b> End <b>0.8</b><br>
      QR Pattern: Weight <b>0.10</b> Start <b>0.4</b> End <b>0.8</b>
    </figcaption>
  </figure>
  <div />
</div>
<blockquote>
<p><span i-ri-lightbulb-line text-yellow/> If you didn't see the tabs for multiple layers of ControlNet, you can go to the settings page to enable it:<br>
<img src="/images/ai-qrcode-101-multi-cn-settings.png" alt=""></p>
</blockquote>
<hr>
<div border="~ rounded-full base" px3 py1 inline text-sm float-right>
<span i-ri-book-2-line /> Credits to wangcai
</div>
<h3>OpenPose</h3>
<p>To get more control over the composition, you can also use other ControlNet models like OpenPose to generate a human pose and use it as the input of the QR Code.</p>
<p>For example, you can see the following image is generated with both QR Code and OpenPose as the input. With some tricks on the composition, you can shift the focus of the viewers to the other parts of the image and make the QR Code less obvious.</p>
<div flex="~ col items-center" py4>
  <QRCodeCompare scale-85 md:scale-100 h-80
    input="/images/ai-qrcode-101-openpose-qr.png"
    input2="/images/ai-qrcode-101-openpose-pose.png"
    output="/images/ai-qrcode-101-openpose-output1.jpg" 
  />
<!-- 
  <QRCodeCompare scale-85 md:scale-100 h-80
    input="/images/ai-qrcode-101-openpose-qr.png"
    input2="/images/ai-qrcode-101-openpose-pose.png"
    output="/images/ai-qrcode-101-openpose-output2.jpg" 
  /> -->
<p><QRCodeCompare scale-85 md:scale-100 h-80 mt4
    input="/images/ai-qrcode-101-openpose-qr3.png"
    input2="/images/ai-qrcode-101-openpose-pose3.png"
    output="/images/ai-qrcode-101-openpose-output4.png" 
  /></p>
<p><QRCodeCompare scale-85 md:scale-100 h-80
    input="/images/ai-qrcode-101-openpose-qr2.png"
    input2="/images/ai-qrcode-101-openpose-pose2.png"
    output="/images/ai-qrcode-101-openpose-output3.png" 
  /></p>
</div>
<p>You can learn more about OpenPose in <a href="https://stable-diffusion-art.com/controlnet/">this tutorial</a>.</p>
<hr>
<div border="~ rounded-full base" px3 py1 inline text-sm float-right>
<span i-ri-book-2-line /> Credits to <a href="https://antfu.me" target="_blank">Anthony Fu</a>
</div>
<h3>Selective Multi-layer Control</h3>
<p>Look deep into the <a href="https://en.wikipedia.org/wiki/QR_code#Standards">QR Code specification</a>, you can see a QR Code is composed with different types of data and position patterns:</p>
<p><img src="/images/ai-qrcode-101-qr-struct.png" alt=""></p>
<p>Other than the position markers that are obvious to find, we can see there are also the <strong>Version and Format information</strong> around the position markers. Those information are quite important because it tells the scanner how to decode the QR Code properly. On the other hand, since the <strong>Data area</strong> has good error correction and duplications, it's actually fine for it to contain a few misalignment when needed. Now we realize that many QR Code that are not scannable are because those area are not distinguishable enough, causing the scanner to exit early before going into the actual data.</p>
<p>So, since the data points in a QR Code are <strong>not equally important</strong>, why would we control them equally? Maybe we could try to selective control different areas. Like increasing the control weight of the functional areas and decreasing the weight of the data area, to make the QR Code more scannable while being more artistic.</p>
<p>In the recent update of <a href="https://qrcode.antfu.me/">QR Toolkit</a>, we added a new option <strong>Render Type</strong> to only generate some specific areas of the QR Code, combining with a grey background, we could have:</p>
<blockquote>
<p><span i-ri-lightbulb-line text-yellow/> Both <strong>QR Pattern v2</strong> and <strong>QR Code Monster</strong> models support having grey as the hint of arbitrary content (bypass the control). Thanks for the information from <a href="https://civitai.com/user/Nacholmo">Nacholmo</a> and <a href="https://twitter.com/vyrilbareme">Cyril Bareme</a>.</p>
</blockquote>
<p><img src="/images/ai-qrcode-101-render-type.png" alt=""></p>
<p>With this, we could use two ControlNet layers:</p>
<ul>
<li>Layer 1: Full QR Code, with medium control weight.</li>
<li>Layer 2: Selective parts of QR Code, with <strong>strong weight</strong> but shorter control steps.</li>
</ul>
<p>For example, here I have two ControlNet layers, both using the <strong>QR Code Monster</strong> model:</p>
<QRCodeSelectiveLayers />
<blockquote>
<p><span i-ri-lightbulb-line text-yellow/> In the second layer of the example, I excluded the position markers as I was seeking for more blend-in image. You can also include them if you want to make the QR Code more scannable.</p>
</blockquote>
<p>After a few tweaks, the result are surprisingly good. It's able to retain the recognizability of the QR Code while being more artistic. Here are some of the results:</p>
<div flex="~ col items-center gap-8" py6>
  <QRCodeCompare scale-85 md:scale-100 h-100
    input="/images/ai-qrcode-101-selective-qr1.png"
    input2="/images/ai-qrcode-101-selective-qr2.png"
    output="/images/ai-qrcode-101-selective-example1.jpg" 
  />
<p><QRCodeCompare scale-85 md:scale-100 h-100
    input="/images/ai-qrcode-101-selective-qr1.png"
    input2="/images/ai-qrcode-101-selective-qr2.png"
    output="/images/ai-qrcode-101-selective-example2.jpg" 
  /></p>
<p><QRCodeCompare scale-85 md:scale-100 h-100
    input="/images/ai-qrcode-101-selective-qr1.png"
    input2="/images/ai-qrcode-101-selective-qr2.png"
    output="/images/ai-qrcode-101-selective-example3.jpg" 
  /></p>
</div>
<hr>
<div border="~ rounded-full base" px3 py1 inline text-sm float-right>
<span i-ri-book-2-line /> Credits to <a href="https://huggingface.co/monster-labs/control_v1p_sd15_qrcode_monster#tips" target="_blank">QR Code Monster</a>
</div>
<h3>Image to Image Enhancement</h3>
<p>When you find a generated image is hard to scan, you can try to send the image to <code>img2img</code>, enable ControlNet with your original QR Code input and:</p>
<ul>
<li>Decrease the <strong>Denoising strength</strong> to retain more of the original image.</li>
<li>Increase the <strong>Control weight</strong> for better readability.</li>
<li>A typical workflow for &quot;saving&quot; a code would be: Max out the guidance scale and minimize the denoising strength, then bump the strength until the code scans.</li>
</ul>
<p>This tells the model to re-enhance the image by making dark areas darker and light areas lighter under the guidance of ControlNet.</p>
<hr>
<h3>Manually Editing and Inpainting</h3>
<p>The ultimate solution is indeed to manually edit the output image. You can use editing tools like Photoshop combined with inpainting to fine-tune every part of the imaged image. It might require a lot of effort, we'd generally recommend focusing on tweaking the generation first before going to this step. More details can be found in <a href="/posts/ai-qrcode-refine">this post</a>.</p>
<h2>Extra: Hidden Text in Image</h2>
<p>While the QR Code models are primarily designed for generating QR Codes, they are fundamentally brightness-contrast models. This means that they can be used to control and modify anything that exhibits distinct local contrast. So, in addition to generating QR Codes, we can utilize these models to hide text or any symbols inside the generated images. This opens up exciting possibilities for creative exploration beyond just QR Code generation.</p>
<p>For example, we could have this using the exact same methods we learned for generating QR Code:</p>
<p><img src="/images/ai-qrcode-101-text-result1.jpg" alt=""></p>
<details mt--6>
<summary op50 select-none>Input Image</summary>
<p>
<img src="/images/ai-qrcode-101-text-input1.png" rounded shadow important-mt0 />
</p>
</details>
<p>You can click the image to see the full size. When you zoom in on the QR Code image, it can become challenging to distinguish the text from the background. However, when you zoom out significantly, the text becomes much clearer and easier to scan. This observation highlights an interesting aspect of human vision—our eyes are indeed excellent scanners.</p>
<p>Similarly, we could combing with QR Code, or anything you can think of:</p>
<p><img src="/images/ai-qrcode-101-text-result2.png" alt=""></p>
<details mt--6>
<summary op50 select-none>Input Image</summary>
<p>
<img src="/images/ai-qrcode-101-text-input2.png" rounded shadow important-mt0 />
</p>
</details>
<h2>Contributing</h2>
<p>This guide is aimed to be a one-stop documentations and references for the community to learn about the QR Code models and how to use them.</p>
<p>If you are interested in contributing to this post, fixing typos, or adding new ideas, you can <a href="https://github.com/antfu/antfu.me/edit/main/pages/posts/ai-qrcode-101.md">edit this page on GitHub</a>. Or if you are not familiar with Git, you can also go to <a href="https://chat.antfu.me">Anthony's Discord server</a> and discuss with us.</p>
]]></content:encoded>
            <author>hi@antfu.me (Anthony Fu)</author>
        </item>
        <item>
            <title><![CDATA[Refining AI Generated QR Code]]></title>
            <link>https://antfu.me/posts/ai-qrcode-refine</link>
            <guid>https://antfu.me/posts/ai-qrcode-refine</guid>
            <pubDate>Fri, 30 Jun 2023 17:00:00 GMT</pubDate>
            <content:encoded><![CDATA[<p>[[toc]]</p>
<blockquote>
<p><strong>Update</strong>: New blog posts - <a href="/posts/ai-qrcode-101"><strong>Stable Diffusion QR Code 101</strong></a></p>
</blockquote>
<p>Last week, I wrote a <a href="/posts/ai-qrcode">blog post</a> about how I learned to generate scannable QR Codes. When doing so, I consider my goal is to find an image that looks like a QR Code as little as possible to humans, but still be recognizable by the machine.</p>
<p>We need to find a balance, tweaking the weights to try and error. It's still quite hard to find a good composition that represents the black &amp; white spots, while keeping the content meaningful to human. If you go too far, the QR Code will be unscannable, and if you don't go far enough, the image will just be like a boring QR Code.</p>
<p>Since there is quite some randomness in the process, sometimes it could be a pity when you find a good one but realize it's not scannable. To improve this, my workflow was to open up Photoshop, overlay the generated image with the original QR Code, manually check the difference, use the brush to mark those spots and send to <strong>inpaint</strong> to draw those areas. It works to some extent, but pretty inefficient as you need to go back and forth quite a few times. Meanwhile, doing this manually can also be inaccurate as the scanning algorithm might see them differently.</p>
<p><img src="https://antfu.me/images/ai-qrcode-refine-5.jpg" alt="Steps from QR Code to final image"></p>
<p>So, I need to find a way to automate this, helping me to verify and refine the generated QR Code easier. And I came up with a simple web tool to do so. Let me introduce you to a bit about it.</p>
<div i-ri-arrow-right-line /> <a href="https://qrcode.antfu.me/" target="_blank">Anthony's QR Code Toolkit</a>
<h2>Generating the Base QR Code</h2>
<p>One thing I found quite important is that the generated QR Code we put in the ControlNet affects the image quite a lot. The basic square QR Code will lead to a more square-ish and blocky image. It's worth to try with dots, rounded, or other styled QR Codes to see if they can help to generate a better image.</p>
<p><img src="/images/ai-qrcode-refine-input-compare.jpg" alt="Comparison grid between different styled QR Code as input"></p>
<p>The images above are generated with the exactly same parameters, and the same seed, except the QR Code inputs has slightly different on the styles. You can see the difference is quite significant.</p>
<p>In addition, since the distribution of QR Codes is directly affecting the image's composition. Sometimes we might find some patterns might be hard to work around. We would need to find different versions of the QR Code to find a better fit to the image we want. If you are familiar with QR Code enough, you might know there is a step in QR Code generated called <a href="https://en.wikipedia.org/wiki/QR_code#Encoding">Mask Pattern</a>. There are in total 8 different kind of patterns can apply to the QR Code that serves the same content. Sadly, most of the generators do not provide the capability to change it. Ok, I'll build it.</p>
<p>So specifically for this need, I built a QR generator based on <a href="https://www.nayuki.io/page/qr-code-generator-library">QR Code Generator Library</a>:</p>
<p><img src="/images/ai-qrcode-refine-generate-1.png" alt="QR Code Generator"></p>
<p>It offers me the full capability of the generation process. You can change the error correction level, mask pattern, version of the QR Code, and rotation to <strong>find</strong> a good distribution of the black &amp; white spots**. Also, it allows you to change the styles of the dots, or add some random noise to the border making the generated image more blended-in.</p>
<p><img src="/images/ai-qrcode-refine-generate-2.png" alt="QR Code Generator with Custom Styles"></p>
<h2>Generating the Images</h2>
<p>Now we have the QR Code, we could move up to generate those images with Stable Diffusion and ControlNet. For detailed steps, please refer to <a href="/posts/ai-qrcode">my previous blog post</a>.</p>
<h2>Verify and Refine the QR Code</h2>
<p>Running overnight, I now got like 200 images generated. Say I find one quite interesting and see some potential of being a good one. I will first use my phone to try to scan it. As mentioned earlier, you may not get lucky every time. This one is unfortunately not scannable.</p>
<p align="center">
<img src="/images/ai-qrcode-refine-4.jpg" class="max-w-120!" alt="Picked one, right from the model" />
</p>
<p>From a glance, we see there are quite some QR Code-ish spots in this image, which should make it recognizable by the scanner. But why not? Let's find out why:</p>
<p>Using the <strong>Compare</strong> tab of the <a href="https://qrcode.antfu.me/">toolkit</a>, upload both the generated image and the original QR Code, tweak the grid size, and then we could see the mismatched spots and inspect the nodes.</p>
<p><img src="/images/ai-qrcode-refine-compare-1.png" alt=""></p>
<p>We can see that the image is not scannable because we have quite a lot of mismatches, saying that some parts of the image might not have enough contrast. Hover on the <strong>Highlight Mismatch</strong> button, we can see the mismatched spots highlighted:</p>
<p><img src="/images/ai-qrcode-refine-compare-2.png" alt=""></p>
<p>It seems the top half part of the image is a bit too dark and makes the scanner hard to distinguish. We can also try to increase the image contrast to see how it would look like in the scanner:</p>
<p><img src="/images/ai-qrcode-refine-compare-3.png" alt=""></p>
<p>Now it's quite clear what's the problem. Then how can we fix it? You can then try to hover on the <strong>Preview Corrected</strong> button, to see what needs to be changed:</p>
<p><img src="/images/ai-qrcode-refine-compare-4.png" alt=""></p>
<p>It will lighten the spots that are too dark, and darken the spots that are too bright. Then you see this image immediately <strong>becomes scannable</strong> now!</p>
<p>It's great but definitely not the final result we would end up with. We can download the correction overlay, or the mask from the toolkit, to use them on <strong>inpaint</strong> or fine-grained adjustment in Photoshop.</p>
<h2>Final</h2>
<p>After a few rounds of inpainting and adjustment, upscale to improve details, and now we have the final image as:</p>
<p align="center">
<img src="/images/ai-qrcode-refine-final.jpg" class="max-w-120!" alt="Final result" />
</p>
<QRNotScannable mt--2 />
<p>Put it back to the toolkit, we see that the mismatched spots are now reduced a lot! Some of the mismatches are actually made on purpose, since QR Code has the error correction capability allowing that.</p>
<p><img src="/images/ai-qrcode-refine-compare-final-1.png" alt="Tge final result in the toolkit"></p>
<p>In case you are interested, here you can see what it looks like when overlaid with the original QR Code:</p>
<p><img src="/images/ai-qrcode-refine-compare-final-2.png" alt="The final result with the original QR Code overlayed on top"></p>
<p>It's quite interesting to see how the QR Code is been distorted and blended as different parts of the image.</p>
<h2>Hide the Markers</h2>
<p>The current result is already surprisingly good to me. The only thing that is missing probably is that the position makers do not blend very well, but I guess that's kinda the limitation. When I was about to call it a day and go to bed, thinking about the possibility of making the QR Code makers less obvious, I saw in <a href="https://classic.qrbtf.com/">classic.qrbtf.com</a> (created by the creator that came up with the AI QR Code idea), there is a style call SP-1 that has a &quot;Plus shape&quot; style of the position markers. It looks much less artificial than the squared or double-circle ones. I didn't know it would also work for the scanner, so I think it might be worth a try.</p>
<p><img src="/images/ai-qrcode-refine-qrbft.png" alt="Styles in classic.qrbtf.com"></p>
<p>So I implemented it in my generator, and it looks like this:</p>
<p><img src="/images/ai-qrcode-refine-plus-sign.png" alt="QR Code generator with plus sign shaped markers"></p>
<p>As you can see, the marker looks much less distinguishable from the other data points (be aware it also make the code less scannable). It might be worth trying as the control net input to see if it can generate better images. But since we already have a pretty good one, let's use the new QR Code to redraw the markers.</p>
<p>So doing the inpainting process again using the new QR Code, and a few more editing, we have the <strong>final result</strong> as:</p>
<p align="center">
<img src="/images/ai-qrcode-refine-no-anchor.png" class="max-w-120!" alt="Final result" />
</p>
<QRNotScannable mt--2 />
<p>Even though I made it step by step, it's still mind-blowing to see the final result looks like this but still scannable! 🤯</p>
<p><a href="https://civitai.com/images/1350374">Check it on Civital</a></p>
<h2>Bonus Tip: Distort the QR</h2>
<p>Since we found the QR Code input affects the output image quite significantly. In another way of thinking, instead of refining the generated image in the post, maybe we can also try to manipulate the QR Code itself before sending it to the model.</p>
<p>For example, we could use the generator to try different patterns and configurations, to generate a better distribution of the data points. Adding some noise in the margin, making the position makers more randomized, and rounding up the hard edges to reduce the blocky feeling. We could have:</p>
<p><img src="/images/ai-qrcode-refine-distort-1.png" alt=""></p>
<p>Then I started to think about what more we could do. So I tried to play filter effects in Photoshop. I found that the <code>Distort &gt; Ripple</code> and <code>Pixelate &gt; Crystallize</code> filters have quite a balanced distortion effect. So I reimplemented the <strong>crystallize</strong> effect in the toolkit, and we have:</p>
<p><img src="/images/ai-qrcode-refine-distort-2.png" alt=""></p>
<p>This further blurs the distinction between data points in human eyes. Sending it to the model, we get surprisingly very good results! Here is one of the examples:</p>
<p align="center">
<img src="/images/ai-qrcode-refine-distort-result.png" class="max-w-120!" alt="Distorted QR Code" />
</p>
<QRNotScannable mt--2 />
<p>Since input has much more soft edges with some shades, it makes the model being able to &quot;guess&quot; with items with more freedom. Hope you'll find this tip useful! I will try to implement more useful effects in the toolkit as we go.</p>
<h2>Conclusion</h2>
<p>I hope you enjoy the walkthrough. If you just started doing AI QR Code, give a try to the tool and let me know if it helps. You can find the app and the source code below.</p>
<div i-ri-qr-code-line /> <a href="https://qrcode.antfu.me/" target="_blank">Anthony's QR Code Toolkit</a><br>
<div i-ri-github-fill /> <a href="https://github.com/antfu/qrcode-toolkit" target="_blank" font-mono>antfu/qrcode-toolkit</a>
<p>Join my <a href="https://chat.antfu.me"><span op75 i-simple-icons-discord /> Discord Server</a>, share what you are working on, and let's explore more together!</p>
<p>If you are interested in how I make such tools, I'd recommend continuing reading <a href="/posts/about-yak-shaving">About</a> Yak Shaving](/posts/about-yak-shaving) to learn the philosophy I follow when building tools. And if you like my work, consider sponsoring on <a href="https://github.com/sponsors/antfu"><span i-carbon-favorite /> GitHub Sponsor</a> or <a href="https://afdian.net/a/antfu"><span i-carbon-lightning /> 爱发电</a> to support me in coming up with more ideas and tools.</p>
<p>Thank you and happy hacking!</p>
]]></content:encoded>
            <author>hi@antfu.me (Anthony Fu)</author>
        </item>
        <item>
            <title><![CDATA[Stylistic QR Code with Stable Diffusion]]></title>
            <link>https://antfu.me/posts/ai-qrcode</link>
            <guid>https://antfu.me/posts/ai-qrcode</guid>
            <pubDate>Sun, 25 Jun 2023 05:00:00 GMT</pubDate>
            <content:encoded><![CDATA[<p>[[toc]]</p>
<blockquote>
<p><strong>Update</strong>: New blog posts</p>
<ul>
<li><a href="/posts/ai-qrcode-refine">👉 <strong>Refining AI Generated QR Code</strong></a></li>
<li><a href="/posts/ai-qrcode-101">📚 <strong>Stable Diffusion QR Code 101</strong></a></li>
</ul>
</blockquote>
<p>Yesterday, I created this image using <a href="https://stability.ai/blog/stable-diffusion-public-release">Stable Diffusion</a> and <a href="https://github.com/lllyasviel/ControlNet">ControlNet</a>, and shared on <a href="https://twitter.com/antfu7/status/1672671149698818048">Twitter</a> and <a href="https://www.instagram.com/p/Ct4fpkgtc1W/">Instagram</a> -- an illustration that also functions as a scannable QR code.</p>
<p><img src="https://antfu.me/images/ai-qrcode-final.jpg" alt=""></p>
<QRNotScannable mt--2 />
<p>The process of creating it was super fun, and I'm quite satisfied with the outcome.</p>
<p>In this post, I would like to share some insights into my learning journey and the approaches I adopted to create this image. Additionally, I want to take this opportunity to credit the remarkable tools and models that made this project possible.</p>
<h2>Get into the Stable Diffusion</h2>
<p>This year has witnessed an explosion of mind-boggling AI technologies, such as <a href="https://chat.openai.com/">ChatGPT</a>, <a href="https://openai.com/dall-e-2">DALL-E</a>, <a href="https://www.midjourney.com/">Midjourney</a>, <a href="https://stability.ai/blog/stable-diffusion-public-release">Stable Diffusion</a>, and many more. As a former photographer also with some interest in design and art, being able to generate images directly from imagination in minutes is undeniably tempting.</p>
<p>So I started by trying Midjourney, it's super easy to use, very expressive, and the quality is actually pretty good. It would honestly be my recommendation for anyone who wants to get started with generative AI art.</p>
<p>By the way, Inès has also delved into it and become quite good at it now, go check her work on her new Instagram account <span op75 i-ri-arrow-right-line /> <a href="https://instagram.com/a.i.nes/">@a.i.nes</a>.</p>
<p>On my end, being a programmer with strong preferences, I would naturally seek for greater control over the process. This brought me to the realm of Stable Diffusion. I started with this guide: <a href="https://aituts.com/stable-diffusion-lora/"><em>Stable Diffusion LoRA Models: A Complete Guide</em></a>. The benefit of being late to the party is that there are already a lot of tools and guides ready to use. Setting up the environment quite straightforward and luckily my M1 Max's GPU is supported.</p>
<h2>QR Code Image</h2>
<p>A few weeks ago, <a href="https://www.reddit.com/r/StableDiffusion/comments/141hg9x/controlnet_for_qr_code/"><code>nhciao</code> on reddit posted a series of artistic QR codes</a> created using Stable Diffusion and <a href="https://github.com/lllyasviel/ControlNet">ControlNet</a>. The concept behind them fascinated me, and I defintely want to make one for my own. So I did some research and managed to find the original article in Chinese: <a href="https://mp.weixin.qq.com/s/i4WR5ULH1ZZYl8Watf3EPw">Use AI to Generate Scannable Images</a>. The author provided insights into their motivations and the process of training the model, although they did not release the model itself. On the other hand, they are building a service called <a href="https://qrbtf.com/">QRBTF.AI</a> to generate such QR code, however it is not yet available.</p>
<p>Until another day I found an community model <a href="https://civitai.com/models/90940/controlnet-qr-pattern-qr-codes">QR Pattern Controlnet Model</a> on <a href="https://civitai.com">CivitAI</a>. I know I got to give it a try!</p>
<h2>Setup</h2>
<p>My goal was to generate a QR code image that directs to my website while elements that reflect my interests. I ended up taking a slightly cypherpunk style with a character representing myself :P</p>
<blockquote>
<p><strong>Disclaimer</strong>: I'm certainly far from being an expert in AI or related fields. In this post, I'm simply sharing what I've learned and the process I followed. My understanding may not be entirely accurate, and there are likely optimizations that could simplify the process. If you have any suggestions or comments, please feel free to reach out using the links at the bottom of the page. Thank you!</p>
</blockquote>
<h3>1. Setup Environment</h3>
<p>I pretty much follows <a href="https://aituts.com/stable-diffusion-lora/">Stable Diffusion LoRA Models: A Complete Guide</a> to install the web ui <a href="https://github.com/AUTOMATIC1111/stable-diffusion-webui">AUTOMATIC1111/stable-diffusion-webui</a>, download models you are interested in from <a href="https://civitai.com/models">CivitAI</a>, etc. As a side note, I found that the user experience of the web ui is not super friendly, some of them I guess are a bit architectural issues that might not be easy to improve, but luckily I found a pretty nice theme <a href="https://github.com/canisminor1990/sd-webui-kitchen-theme">canisminor1990/sd-webui-kitchen-theme</a> that improves a bunch of small things.</p>
<p>In order to use ControlNet, you will also need to install the <a href="https://github.com/Mikubill/sd-webui-controlnet">Mikubill/sd-webui-controlnet</a> extension for the web ui.</p>
<p>Then you can download the <a href="https://civitai.com/models/90940/controlnet-qr-pattern-qr-codes">QR Pattern Controlnet Model</a>, putt the two files (<code>.safetensors</code> and <code>.yaml</code>) under <code>stable-diffusion-webui/models/ControlNet</code> folder, and restart the web ui.</p>
<h3>2. Create a QR Code</h3>
<p>There are hundreds of QR Code generators full of adds or paid services, and we certainly don't need those fanciness -- because we are going to make it much more fancier 😝!</p>
<p>So I end up found the <a href="https://www.nayuki.io/page/qr-code-generator-library">QR Code Generator Library</a>, a playground of an open source QR Code generator. It's simple but exactly what I need! It's better to use medium error correction level or above to make it more easy recognizable later. Small tip that you can try with different <strong>Mask pattern</strong> to find a better color destribution that fits your design.</p>
<h3>3. Text to Image</h3>
<p>As the regular Text2Image workflow, we need to provide some prompts for the AI to generate the image from. Here is the prompts I used:</p>
<div>
  <div text-sm op60>Prompts</div>
</div>
<pre><code class="language-txt">(one male engineer), medium curly hair, from side, (mechanics), circuit board, steampunk, machine, studio, table, science fiction, high contrast, high key, cinematic light,
(masterpiece, top quality, best quality, official art, beautiful and aesthetic:1.3), extreme detailed, highest detailed, (ultra-detailed)
</code></pre>
<div>
  <div text-sm op60>Negative Prompts</div>
</div>
<pre><code class="language-txt">(worst quality, low quality:2), overexposure, watermark, text, easynegative, ugly, (blurry:2), bad_prompt,bad-artist, bad hand, ng_deepnegative_v1_75t
</code></pre>
<p><img src="/images/ai-qrcode-t2i.png" alt=""></p>
<p>Then we need to go the ControlNet section, and upload the QR code image we generated earlier. And configure the parameters as suggested in the model homepage.</p>
<p><img src="/images/ai-qrcode-controlnet-config.png" alt=""></p>
<p>Then you can start to generate a few images  and see if it met your expectations. You will also need to check if the generated image is scannable, if not, you can tweak the <strong>Start controling step</strong> and <strong>End controling step</strong> to find a good balance between stylization and QRCode-likeness.</p>
<h3>4. I'm feeling lucky!</h3>
<p>After finding a set of parameters that I am happy with, I will increase the <strong>Batch Count</strong> to around 100 and let the model generate variations randomly. Later I can go through them and pick one with the best conposition and details for further refinement. This can take a lot of time, and also a lot of resources from your processors. So I usually start it before going to bed and leave it overnight.</p>
<p>Here are some examples of the generated variations (not all of them are scannable):</p>
<p><img src="/images/ai-qrcode-examples-grid.png" alt="Generation Examples"></p>
<p>From approximately one hundred variations, I ultimately chose the following image as the starting point:</p>
<p><img src="/images/ai-qrcode-original.jpg" alt="Original QR Code Image"></p>
<p>It gets pretty interesting composition, while being less obvious as a QR code. So I decided to proceed with it and add add a bit more details. (You can compare it with the final result to see the changes I made.)</p>
<h3>5. Refining Details</h3>
<blockquote>
<p>Update: I recently built a toolkit to help with this process, check my new blog post <a href="/posts/ai-qrcode-refine">👉 <strong>Refine AI Generated QR Code</strong></a> for more details.</p>
</blockquote>
<p>The generated images from the model are not perfect in every detail. For instance, you may have noticed that the hand and face appear slightly distorted, and the three anchor boxes in the corner are less visually appealing. We can use the <strong>inpaint</strong> feature to tell the model to redraw some parts of the image (it would better if you keep the same or similiar prompts as the original generation).</p>
<p>Inpainting typically requires a similar amount of time as generating a text-to-image, and it involves either luck or patience. Often, I utilize Photoshop to &quot;borrow&quot; some parts from previously generated images and utilize the spot healing brush tool to clean up glitches and artifacts. My Photoshop layers would looks like this:</p>
<img src="/images/ai-qrcode-ps-layers.png" alt="Photoshop Layers" class="w-100! mxa"/>
<p>After making these adjustments, I'll send the combined image back for inpainting again to ensure a more seamless blend. Or to search for some other components that I didn't found in other images.</p>
<p>Specifically on the QR Code, in some cases ControlNet may not have enough prioritize, causing the prompts to take over and result in certain parts of the QR Code not matching. To address this, I would overlay the original QR Code image onto the generated image (as shown in the left image below), identify any mismatches, and use a brush tool to paint those parts with the correct colors (as shown in the right image below).</p>
<p><img src="/images/ai-qrcode-overlay-inpaint.png" alt="Overlaying QR Code"></p>
<p>I then export the marked image for inpainting once again, adjusting the <strong>Denoising strength</strong> to approximately 0.7. This would ensures that the model overrides our marks while still respecting the color to some degree.</p>
<p>Ultimately, I iterate through this process multiple times until I am satisfied with every detail.</p>
<h3>6. Upscaling</h3>
<p>The recommended generation size is 920x920 pixels. However, the model does not always generate highly detailed results at the pixel level. As a result, details like the face and hands can appear blurry when they are too small. To overcome this, we can upscale the image, providing the model with more pixels to work with. The <code>SD Upscaler</code> script in the <code>img2img</code> tab is particularly effective for this purpose. You can refer to the guide <a href="https://easywithai.com/guide/how-to-use-upscalers-in-stable-diffusion/">Upscale Images With Stable Diffusion</a> for more information.</p>
<p><img src="/images/ai-qrcode-upscale.png" alt=""></p>
<h3>7. Post-processing</h3>
<p>Lastly, I use Photship and Lightroom for subtle color grading and post-processing, and we are done!</p>
<p><img src="/images/ai-qrcode-final.jpg" alt="Final QR Code Image"></p>
<p>The one I end up with not very good error tolerance, you might need to try a few times or use a more forgiving scanner to get it scanned :P</p>
<p>And using the similarly process, I made another one for Inès:</p>
<p><img src="/images/ai-qrcode-ines.png" alt="Inès' QR Code Image"></p>
<h2>Conclusion</h2>
<p>Creating this image took me a full day, with a total of 10 hours of learning, generating, and refining. The process was incredibly enjoyable for me, and I am thrilled with the end result! I hope this post can offer you some fundamental concepts or inspire you to embark on your own creative journey. There is undoubtedly much more to explore in this field, and I eager to see what's coming next!</p>
<p>Join my <a href="https://chat.antfu.me">Discord Server</a> and let's explore more together!</p>
<p>If you want to learn more about the refining process, go check my new blog post: <a href="/posts/ai-qrcode-refine"><strong>Refining AI Generated QR Code</strong></a>.</p>
<h2>References</h2>
<p>Here are the list of resources for easier reference.</p>
<h3>Concepts</h3>
<ul>
<li><a href="https://stability.ai/blog/stable-diffusion-public-release">Stable Diffusion</a></li>
<li><a href="https://github.com/lllyasviel/ControlNet">ControlNet</a></li>
</ul>
<h3>Tools</h3>
<ul>
<li><a href="https://github.com/antfu/use">Hardwares &amp; Softwares I am using</a>.</li>
<li><a href="https://github.com/AUTOMATIC1111/stable-diffusion-webui">AUTOMATIC1111/stable-diffusion-webui</a> - Web UI for Stable Diffusion
<ul>
<li><a href="https://github.com/canisminor1990/sd-webui-kitchen-theme">canisminor1990/sd-webui-kitchen-theme</a> - Nice UI enhancement</li>
</ul>
</li>
<li><a href="https://github.com/Mikubill/sd-webui-controlnet">Mikubill/sd-webui-controlnet</a> - ControlNet extension for the webui</li>
<li><a href="https://www.nayuki.io/page/qr-code-generator-library">QR Code Generator Library</a> - QR code generator that is ad-free and customisable</li>
<li><a href="https://www.adobe.com/products/photoshop.html">Adobe Photoshop</a> - The tool I used to blend the QR code and the illustration</li>
</ul>
<h3>Models</h3>
<ul>
<li>Control Net Models for QR Code (you can pick one of them)
<ul>
<li><a href="https://civitai.com/models/90940/controlnet-qr-pattern-qr-codes">QR Pattern Controlnet Model</a></li>
<li><a href="https://huggingface.co/monster-labs/control_v1p_sd15_qrcode_monster">Controlnet QR Code Monster</a></li>
<li><a href="https://huggingface.co/ioclab/ioc-controlnet/tree/main/models">IoC Lab Control Net</a></li>
</ul>
</li>
<li>Checkpoint Model (you can use any checkpoints you like)
<ul>
<li><a href="https://civitai.com/models/36520/ghostmix">Ghostmix Checkpoint</a> - A very high quality checkpoint I use. You can use any other checkpoints you like</li>
</ul>
</li>
</ul>
<h3>Tutorials</h3>
<ul>
<li><a href="https://aituts.com/stable-diffusion-lora/">Stable Diffusion LoRA Models: A Complete Guide</a> - The one I used to get started</li>
<li><a href="https://mp.weixin.qq.com/s/i4WR5ULH1ZZYl8Watf3EPw">(Chinese) Use AI to genereate scannable images</a> - Unfortunately the article is in Chinese and I didn't find a English version of it.</li>
<li><a href="https://easywithai.com/guide/how-to-use-upscalers-in-stable-diffusion/">Upscale Images With Stable Diffusion</a> - Enlarge the image while adding more details</li>
</ul>
]]></content:encoded>
            <author>hi@antfu.me (Anthony Fu)</author>
        </item>
        <item>
            <title><![CDATA[Break Lines in JS]]></title>
            <link>https://antfu.me/posts/break-lines-in-js</link>
            <guid>https://antfu.me/posts/break-lines-in-js</guid>
            <pubDate>Fri, 10 Feb 2023 16:00:00 GMT</pubDate>
            <content:encoded><![CDATA[<p>You probably don't need it usually, but in case you want to break lines programmatically in JavaScript, here is my lazy man's solution:</p>
<!-- eslint-skip -->
<pre><code class="language-js">// break lines at space with maximum 25 characters per line
text.split(/(.{0,25})(?:\s|$)/g).filter(Boolean)
</code></pre>
<p>A quick example:</p>
<pre><code class="language-js">const text = 'A quick brown fox jumps over the lazy dog.'
const lines = text.split(/(.{0,16})(?:\s|$)/g).filter(Boolean)

console.log(lines)
// ['A quick brown', 'fox jumps over', 'the lazy dog.']
</code></pre>
<p>You probably see a few edge cases already. But come on, it's just one line of code :P</p>
]]></content:encoded>
            <author>hi@antfu.me (Anthony Fu)</author>
        </item>
        <item>
            <title><![CDATA[Bonjour Paris!]]></title>
            <link>https://antfu.me/posts/bonjour-paris</link>
            <guid>https://antfu.me/posts/bonjour-paris</guid>
            <pubDate>Sat, 10 Dec 2022 16:00:00 GMT</pubDate>
            <content:encoded><![CDATA[<p>Bonjour Paris!</p>
<p>I am so excited to share with you that we (my GF Inès and me) have <strong>moved to Paris</strong>!</p>
<p>Paris is always our dream city, it's still a bit unreal to see that we are actually here! Here I am going to share with you some of our life updates and what would they mean to us.</p>
<figure>
  <img src="https://antfu.me/images/a-paris-1.jpg" alt="Inès holding a newspaper by @old.press" />
  <figcaption>Inès holding a newspaper by <a href="https://www.instagram.com/old.press" target="_blank">@old.press</a></figcaption>
</figure>
<h2>Thanks</h2>
<p>At the very first, I need to give a <strong>HUGE thanks</strong> to my company <a href="https://nuxtlabs.com/"><strong>NuxtLabs</strong></a> and my colleagues. They made all this possible and helped us a lot along the way.</p>
<p>And also thanks to every one of you. It's your support that helps me find my work valuable and continue working on them. You couldn't imagine my mom and my friends' reactions when I first told them my journey of getting a career by typing on the keyboard at home - the magic of Open Source and the communities! More than that, I got to know so many of you - my friends, colleagues, teams, contributors, followers or fans. All of these really changed our life, thank you!</p>
<h2>Life</h2>
<p>Before coming to France, we were digital nomads for 1.5 years in China. It was a really special experience for us, that we also been through quite a lot of ups and downs. It's fun and fresh to wake up in different places every day, but it also comes with a lot of challenges and trade-offs. You got to get familiar with the new environment every day, and meet new friends but also need to say goodbye very soon.</p>
<p>This time we are going to stay in Paris for a long while. Learning the language, exploring the city, meeting new people and getting a cozy home. It's a new chapter for us, we are so excited about it!</p>
<figure>
  <img src="/images/a-paris-5.jpg" alt="Rives de la seine - by Inès" />
  <figcaption>Rives de la seine - by <a href="https://www.instagram.com/iiiiiiines__/" target="_blank">Inès</a></figcaption>
</figure>
<h2>Photography</h2>
<p>I was quite passionate about photography before I started working on Open Source. Since my focus shifted to programming, I took fewer photos, which is the thing I always feel a bit pity. I think this could be a good chance for me to get back to it.</p>
<figure>
  <img src="/images/a-paris-2.jpg" alt="Le métro" />
  <figcaption>Le métro</figcaption>
</figure>
<p>Instead of carrying a heavy DSLR with lenses, I got a Ricoh GR3x that can easily fit in my pocket. Using it for a few weeks, I found myself becoming more willing to carry it and take photos every day.</p>
<p>I will share more photos on <a href="https://instagram.com/antfu7/">my Instagram</a>. You should also follow <a href="https://instagram.com/iiiiiiines__/">Inès' Instagram</a>, who is a real photographer! :p</p>
<h2>Travel</h2>
<p>So happy the pandemic is close to the end, and I can finally join you in conferences and meetups!</p>
<p>Oh by the way, <strong>I am going to <a href="https://vuejs.amsterdam/">Vue Amsterdam on February 9-10</a> in person</strong>! It would be my first-ever in-person speaking. I am so excited to meet you there!</p>
<p>Once we are settled down, we plan to travel around Europe and visit some of our friends. I will keep you updated on my <a href="https://m.webtoo.ls/@antfu">Mastodon</a> and <a href="https://twitter.com/antfu7">Twitter</a>, let's have a cup of coffee if you are around!</p>
<figure>
  <img src="/images/a-paris-3.jpg" alt="Saint-Antoine des Quinze-Vingts" />
  <figcaption>Saint-Antoine des Quinze-Vingts</figcaption>
</figure>
<figure>
  <img src="/images/a-paris-4.jpg" alt="Rue de Charenton" />
  <figcaption>Rue de Charenton</figcaption>
</figure>
<p>We have been in Paris for only a week, and we are already in love with it! We are so excited about what is coming next to share with you!</p>
]]></content:encoded>
            <author>hi@antfu.me (Anthony Fu)</author>
            <enclosure url="https://antfu.me/images/a-paris-1.jpg" length="0" type="image/jpg"/>
        </item>
        <item>
            <title><![CDATA[Introduction to Vitest - Vue.js Nation 2022]]></title>
            <link>https://antfu.me/posts/introduction-to-vitest-vue-nation-2022</link>
            <guid>https://antfu.me/posts/introduction-to-vitest-vue-nation-2022</guid>
            <pubDate>Wed, 26 Jan 2022 08:00:00 GMT</pubDate>
            <description><![CDATA[Introduction to Vitest - Vue.js Nation 2022]]></description>
            <content:encoded><![CDATA[<blockquote>
<p>Slides: <a href="https://antfu.me/talks/2022-01-26">PDF</a> | <a href="https://talks.antfu.me/2022/vue-nation">SPA</a></p>
<p>Recording: <a href="https://www.youtube.com/watch?v=CW9uTys0li0">YouTube</a></p>
<p>Made with <Slidev class="inline"/>  <a href="https://github.com/slidevjs/slidev"><strong>Slidev</strong></a> - presentation slides for developers.</p>
</blockquote>
]]></content:encoded>
            <author>hi@antfu.me (Anthony Fu)</author>
        </item>
        <item>
            <title><![CDATA[Icons in Pure CSS]]></title>
            <link>https://antfu.me/posts/icons-in-pure-css</link>
            <guid>https://antfu.me/posts/icons-in-pure-css</guid>
            <pubDate>Sun, 31 Oct 2021 16:00:00 GMT</pubDate>
            <description><![CDATA[The icon solution in pure CSS.]]></description>
            <content:encoded><![CDATA[<p>[[toc]]</p>
<blockquote>
<p><a href="/posts/icons-in-pure-css-zh">中文 Chinese Version</a></p>
</blockquote>
<p>In my previous post about <a href="/posts/reimagine-atomic-css#pure-css-icons">Reimagine Atomic CSS</a>, I introduced a preset of <a href="https://github.com/antfu/unocss">UnoCSS</a> that provides the ability to <strong>use any icons on-demand in purely CSS</strong>. Today in this post, I'd like to share with you how we made it possible.</p>
<h2>My Icon Explorations</h2>
<p>If you are interested in how I get here, there is an index of my previous post about the stories of my icon explorations and experiments.</p>
<ul>
<li>Aug. 2020 - <a href="/posts/journey-with-icons">Journey with Icons</a></li>
<li>Sep. 2021 - <a href="/posts/journey-with-icons-continues">Journey with Icons Continues</a></li>
<li>Oct. 2021 - <a href="/posts/reimagine-atomic-css#pure-css-icons">Reimagine Atomic CSS (The CSS Icons Preset)</a></li>
<li>Nov. 2021 - Icons in Pure CSS - <em>you are here!</em></li>
</ul>
<h2>Prior Arts</h2>
<p>I know there is a Pure CSS icon solution called <a href="https://github.com/astrit/css.gg"><code>css.gg</code></a>, which is a great idea to use pseudo-elements (<code>::before</code>, <code>::after</code>) to construct the icons. However, that could require some expert knowledge of how CSS works, but I imagine that approach could be hard to create more complex icons. Instead of the limited choices in a specific set, I am seeking <strong>a more general solution that could apply to any icons</strong>.</p>
<h2>The Idea</h2>
<p>The idea come from <a href="https://github.com/antfu/unplugin-icons/issues/88">this feature request</a> created by <a href="https://github.com/husayt">@husayt</a> to <code>unplugin-icons</code> and the initial implementation in <a href="https://github.com/antfu/unplugin-icons/pull/90">this pull request</a> by <a href="https://github.com/userquin">@userquin</a>. The idea here is quite straightforward - to generate CSS with the icons in <a href="https://developer.mozilla.org/en-US/docs/Web/HTTP/Basics_of_HTTP/Data_URIs">DataURI</a> as the background image.</p>
<pre><code class="language-css">.my-icon {
  background: url(data:...) no-repeat center;
  background-color: transparent;
  background-size: 16px 16px;
  height: 16px;
  width: 16px;
  display: inline-block;
}
</code></pre>
<p>With that, we could use any images inlined in CSS with a single class.</p>
<div grid="~ cols-2">
<pre><code class="language-html">&lt;div class=&quot;my-icon&quot;&gt;&lt;/div&gt; 
</code></pre>
<div i-twemoji-grinning-face text-5xl my-auto mx-4 />
</div>
<p>It's indeed an interesting idea. However, this is more like an image instead of an icon. To me, an icon has to be scalable and colorable (if it's monochrome).</p>
<h2>Make it Work</h2>
<h3>DataURI</h3>
<p>Thanks again to <a href="https://iconify.design/">Iconify</a>, which unified 100+ icon sets with 10,000+ icons into <a href="https://github.com/iconify/collections-json">the consistent JSON format</a>. It allows us to get the SVG of any icon set by simply providing the collection and icon ids. The usage is like this:</p>
<pre><code class="language-ts">import { iconToSVG, getIconData } from '@iconify/utils'

const svg = iconToSVG(getIconData('mdi', 'alarm'))
// (this is not the exact API, simplified here for demo)
</code></pre>
<p>Once we got the SVG string, we could convert the it to DataURI:</p>
<pre><code class="language-ts">const dataUri = `data:image/svg+xml;base64,${Buffer.from(svg).toString('base64')}`
</code></pre>
<p>Talking about DataURI, it's almost the default choice to use <a href="https://developer.mozilla.org/en-US/docs/Glossary/Base64">Base64</a> until I read <a href="https://css-tricks.com/probably-dont-base64-svg/">Probably Don't Base64 SVG</a> by Chris Coyier. Base64 is needed to encode binary data like images to be used in plain text files like CSS, while for SVG, since it's already in text format, the extra encoding to Base64 actually makes the file size larger.</p>
<p>Combine the technique mentioned in <a href="https://codepen.io/Tigt/post/optimizing-svgs-in-data-uris">Optimizing SVGs in data URIs</a> by Taylor Hunt to improve the output size, further, here is the solution we end up with.</p>
<pre><code class="language-ts">// https://bl.ocks.org/jennyknuth/222825e315d45a738ed9d6e04c7a88d0
function encodeSvg(svg: string) {
  return svg.replace('&lt;svg', (~svg.indexOf('xmlns') ? '&lt;svg' : '&lt;svg xmlns=&quot;http://www.w3.org/2000/svg&quot;'))
    .replace(/&quot;/g, '\'')
    .replace(/%/g, '%25')
    .replace(/#/g, '%23')
    .replace(/{/g, '%7B')
    .replace(/}/g, '%7D')
    .replace(/&lt;/g, '%3C')
    .replace(/&gt;/g, '%3E')
}

const dataUri = `data:image/svg+xml;utf8,${encodeSvg(svg)}`
</code></pre>
<h3>Scalable</h3>
<p>The first step of making the &quot;image&quot; more like an icon, we need to make it scalable to the context.</p>
<p>Luckily we have the first-class support scaling support - the <code>em</code> unit.</p>
<pre><code class="language-css">.my-icon {
  background: url(data:...) no-repeat center;
  background-color: transparent;
  background-size: 100% 100%;
  height: 1em;
  width: 1em;
}
</code></pre>
<p>By changing the <code>height</code> and <code>width</code> to <code>1em</code>, and the <code>background-size</code> to <code>100%</code>, we made the image scales based on the parent's font size.</p>
<ul>
<li><span text-sm>Small <span inline-block vertical-text-bottom i-ri-bike-line></span></span></li>
<li><span text-base>Normal <span inline-block vertical-text-bottom i-ri-bike-line></span></span></li>
<li><span text-xl>Large <span inline-block vertical-text-bottom i-ri-bike-line></span></span></li>
</ul>
<h3>Colorable</h3>
<p>In inlined SVG, we could use <a href="https://www.w3.org/TR/css-color-3/#currentcolor"><code>fill=&quot;currentColor&quot;</code></a> to make the color of the SVG matches with the current text color. However, when we use it as a background image, it becomes a flat image. The dynamic parts of the SVG are lost, so is the <code>currentColor</code> magic (it's just like you can't override the color of a PNG).</p>
<p>If you do a quick search, you will find that most people are telling you that you can't. Some might offer you the option to assign the colors in the SVG before converting to DataURI, which could solve the specific problem that you want the icon to have color, but not the root cause that the color is not reactive to the context.</p>
<p>Then you might come up with the idea of using <a href="https://developer.mozilla.org/en-US/docs/Web/CSS/filter">CSS filters</a>, like Una Kravets mentioned in <a href="https://css-tricks.com/solved-with-css-colorizing-svg-backgrounds/">Solved with CSS! Colorizing SVG Backgrounds</a>. That sounds valid, but only that you need to calculate the matrix of how to transform the color to the desired ones. Probably feasible by introducing some runtime JavaScript for that? Maybe, if so, we lost the whole point of trying icons in pure CSS.</p>
<p>This sounds like a dead-end to me. Until I accidentally found the article <a href="https://codepen.io/noahblon/post/coloring-svgs-in-css-background-images">Coloring SVGs in CSS Background Images</a> by Noah Blon. In the article, Noah mentioned a brilliant idea of using <a href="https://developer.mozilla.org/en-US/docs/Web/CSS/mask">CSS masks</a> - a property that I have never heard of before.</p>
<pre><code class="language-css">.my-icon {
  background-color: red;
  mask-image: url(icon.svg);
}
</code></pre>
<p>Instead of using the icon as a background image and figuring out a way to color it, we could actually use the icon as a mask to clip the filled background color. Furthermore, we could now use the <code>currentColor</code> magic to have the icon matching with the parent text color!</p>
<pre><code class="language-css">.my-icon {
  background-color: currentColor;
  mask-image: url(icon.svg);
}
</code></pre>
<div pt-4 />
<div text-sky text-xl>This is a blue text, with the blue icon <div i-uil-cloud-showers-heavy /><div i-uil:wind /></div>
<div text-lime text-xl>Green <div i-uil:trees /><div i-uil:desert /></div>
<div text-orange text-xl>Orange <div i-uil:restaurant /><div i-uil:store-alt /></div>
<h3>Icons with Colors</h3>
<p>We made the monochrome icons colorable but now it problem comes to the icons with colors. With the mask approach, the colors and content of the icons got lost, for example:</p>
<div text-4xl inline-flex gap-2 py-4 px-8 bg-gray-400:15 rounded>
<div text-base my-auto>Icon:</div>
<div i-twemoji:astonished-face />
<div text-base my-auto ml-4>Masked:</div>
<div i-ph:circle-fill style="transform: scale(1.3)" />
</div>
<p>Yes, I might say it's hard for one approach to cover all the cases.</p>
<p>Unless - you could <strong>blend two approaches into one</strong>! Remember we just talked about the background image approach serving the icons as images? Isn't that just what we want for colored icons? - We don't need to change the colors after all!</p>
<p>So the solution is actually pretty simple, we just need to find a way to distinguish the monochrome and colored icons smartly. Luckily, since we had access the the SVG content, we could have:</p>
<pre><code class="language-ts">// if an SVG icon have the `currentColor` value,
// it's very likely to be a monochrome icon
const mode = svg.includes('currentColor')
  ? 'mask'
  : 'background-img'

const uri = `url(&quot;data:image/svg+xml;utf8,${encodeSvg(svg)}&quot;)`

// monochrome
if (mode === 'mask') {
  return {
    'mask': `${uri} no-repeat`,
    'mask-size': '100% 100%',
    'background-color': 'currentColor',
    'height': '1em',
    'width': '1em',
  }
}
// colored
else {
  return {
    'background': `${uri} no-repeat`,
    'background-size': '100% 100%',
    'background-color': 'transparent',
    'height': '1em',
    'width': '1em',
  }
}
</code></pre>
<p>And it works surprisingly well! You know, it's now behavior similar to the thing we are using daily - system's native emojis. The color of texts changes based on the context, while emojis stay the colors of their own.</p>
<p>Here are some showcases of what we end up with:</p>
<div text-xl all:mx-1 all:my-2 all:vertical-middle>
<p><span op60 text-sm inline-block w-40 text-right>Material Design</span> <div i-ic:baseline-account-circle /> <div i-ic:baseline-card-membership /> <div i-ic:baseline-verified text-green5 /> <div i-ic:outline-explore text-sky5 /><br>
<br><span op60 text-sm inline-block w-40 text-right>Carbon</span> <div i-carbon:chart-multitype /> <div i-carbon:network-4 /> <div i-carbon:wind-gusts /> <div i-carbon:collaborate /><br>
<br><span op60 text-sm inline-block w-40 text-right>Tabler</span> <div i-tabler:building-carousel /> <div i-tabler:circle-square /> <div i-tabler:color-swatch /> <div i-tabler:cut /><br>
<br><span op60 text-sm inline-block w-40 text-right>Twemoji</span> <div i-twemoji:grinning-face-with-smiling-eyes /> <div i-twemoji:face-in-clouds /> <div i-twemoji:weary-cat /> <div i-twemoji:teacup-without-handle /><br>
<br><span op60 text-sm inline-block w-40 text-right>Logos</span> <div i-logos:vue /> <div i-logos:blender /> <div i-logos:chrome /> <div i-logos:codepen-icon /></p>
</div>
<p>To see and find all the icons available, you can check out my other project <a href="https://icones.js.org/">Icônes</a>.</p>
<h2>Use It</h2>
<p>If you want to try this icons solution in your project, you can install <a href="https://github.com/antfu/unocss">UnoCSS</a> and the icons preset:</p>
<pre><code class="language-bash">npm i -D unocss @unocss/preset-icons @iconify/json
</code></pre>
<p><code>@iconify/json</code> is the package that stores the icon data from Iconify. Alternatively, you could install per icon set, for example, <code>@iconify-json/mdi</code> for Material Design Icons or <code>@iconify-json/carbon</code> for Carbon Icons and so on.</p>
<p>Then in your <code>vite.config.js</code></p>
<pre><code class="language-ts">import { defineConfig } from 'vite'
import UnoCSS from 'unocss'
import UnocssIcons from '@unocss/preset-icons'

export default defineConfig({
  plugins: [
    UnoCSS({
      // when `presets` is specified, the default preset will be disabled
      // so you could only use the pure CSS icons in addition to your
      // existing app without polluting other CSS 
      presets: [
        UnocssIcons({
          // options
          prefix: 'i-',
          extraProperties: {
            display: 'inline-block'
          }
        }),
        // presetUno() - if you want to use other atomic CSS as well
      ],
    }),
  ],
})
</code></pre>
<p>And that's it for today. Hope you enjoy this icons solution from UnoCSS, or get some inspiration from it for your own projects.</p>
<p>Thanks for reading, and see you :)</p>
]]></content:encoded>
            <author>hi@antfu.me (Anthony Fu)</author>
        </item>
        <item>
            <title><![CDATA[Isomorphic `__dirname`]]></title>
            <link>https://antfu.me/posts/isomorphic-dirname copy</link>
            <guid>https://antfu.me/posts/isomorphic-dirname copy</guid>
            <pubDate>Mon, 30 Aug 2021 16:00:00 GMT</pubDate>
            <content:encoded><![CDATA[<p>In ESM, you might found your old friends <code>__dirname</code> and <code>__filename</code> are no longer available. When you search for <a href="https://stackoverflow.com/questions/46745014/alternative-for-dirname-in-node-when-using-the-experimental-modules-flag">solutions</a>, you will find that you will need to parse <code>import.meta.url</code> to get the equivalents. While most of the solutions only show you the way to get them in ESM only, If you like me, who write modules in TypeScript and transpile to both CJS and ESM at the same time using tools like <a href="https://tsup.egoist.sh/"><code>tsup</code></a>. Here is the isomorphic solution:</p>
<pre><code class="language-js">import { dirname } from 'node:path'
import { fileURLToPath } from 'node:url'

const _dirname = typeof __dirname !== 'undefined'
  ? __dirname
  : dirname(fileURLToPath(import.meta.url))
</code></pre>
]]></content:encoded>
            <author>hi@antfu.me (Anthony Fu)</author>
        </item>
        <item>
            <title><![CDATA[通过type：note，添加nodte页面]]></title>
            <link>https://antfu.me/posts/isomorphic-dirname</link>
            <guid>https://antfu.me/posts/isomorphic-dirname</guid>
            <pubDate>Mon, 30 Aug 2021 16:00:00 GMT</pubDate>
            <content:encoded><![CDATA[<p>In ESM, you might found your old friends <code>__dirname</code> and <code>__filename</code> are no longer available. When you search for <a href="https://stackoverflow.com/questions/46745014/alternative-for-dirname-in-node-when-using-the-experimental-modules-flag">solutions</a>, you will find that you will need to parse <code>import.meta.url</code> to get the equivalents. While most of the solutions only show you the way to get them in ESM only, If you like me, who write modules in TypeScript and transpile to both CJS and ESM at the same time using tools like <a href="https://tsup.egoist.sh/"><code>tsup</code></a>. Here is the isomorphic solution:</p>
<pre><code class="language-js">import { dirname } from 'node:path'
import { fileURLToPath } from 'node:url'

const _dirname = typeof __dirname !== 'undefined'
  ? __dirname
  : dirname(fileURLToPath(import.meta.url))
</code></pre>
]]></content:encoded>
            <author>hi@antfu.me (Anthony Fu)</author>
        </item>
        <item>
            <title><![CDATA[Async with Composition API]]></title>
            <link>https://antfu.me/posts/async-with-composition-api</link>
            <guid>https://antfu.me/posts/async-with-composition-api</guid>
            <pubDate>Fri, 16 Jul 2021 08:00:00 GMT</pubDate>
            <description><![CDATA[Notes about the caveat when using async functions in Vue Composition API.]]></description>
            <content:encoded><![CDATA[<p>There is a major caveat when working with asynchronous functions in Vue Composition API, that I believe many of you have ever come across. I have acknowledged it for a while from somewhere, but every time I want to have a detailed reference and share to others, I can't find it's documented anywhere. So, I am thinking about writing one, with a detailed explanation while sorting out the possible solutions for you.</p>
<ul>
<li><a href="#the-problem">The Problem</a></li>
<li><a href="#the-mechanism">The Mechanism</a></li>
<li><a href="#the-limitation">The Limitation</a></li>
<li><a href="#the-solutions">The Solutions</a></li>
</ul>
<h2>The Problem</h2>
<p>When using asynchronous <code>setup()</code>, <strong>you have to use effects and lifecycle hooks before the first <code>await</code> statement.</strong> (<a href="https://github.com/vuejs/rfcs/discussions/234">details</a>)</p>
<p>For example:</p>
<pre><code class="language-ts">import { ref, watch, onMounted, onUnmounted } from 'vue'

export default defineAsyncComponent({
  async setup() {
    const counter = ref(0)

    watch(counter, () =&gt; console.log(counter.value))

    // OK!
    onMounted(() =&gt; console.log('Mounted'))

    // the await statement
    await someAsyncFunction() // &lt;-----------

    // does NOT work!
    onUnmounted(() =&gt; console.log('Unmounted'))

    // still works, but does not auto-dispose 
    // after the component is destroyed (memory leak!)
    watch(counter, () =&gt; console.log(counter.value * 2))
  }
})
</code></pre>
<p>After the <code>await</code> statement,</p>
<p>the following functions will be <strong>limited</strong> (no auto-dispose):</p>
<ul>
<li><code>watch</code> / <code>watchEffect</code></li>
<li><code>computed</code></li>
<li><code>effect</code></li>
</ul>
<p>the following functions will <strong>not work</strong>:</p>
<ul>
<li><code>onMounted</code> / <code>onUnmounted</code> / <code>onXXX</code></li>
<li><code>provide</code> / <code>inject</code></li>
<li><code>getCurrentInstance</code></li>
<li>...</li>
</ul>
<h2>The Mechanism</h2>
<p>Let's take the <code>onMounted</code> API as an example. As we know, <code>onMounted</code> is a hook that registers a listener when the current component gets mounted. Notice that <code>onMounted</code> (along with other composition APIs) are <strong>global</strong>, for what I mean &quot;global&quot; is that it can be imported and called anywhere - there is <strong>no local context</strong> bound to it.</p>
<pre><code class="language-ts">// local: `onMounted` is a method of `component` that bound to it
component.onMounted(/* ... */)

// global: `onMounted` can be called without context
onMounted(/* ... */)
</code></pre>
<p>So, how does <code>onMounted</code> know what component is being mounted?</p>
<p>Vue takes an interesting approach to solve this. It uses an internal variable to record the current component instance. There is a simplified code:</p>
<p>When Vue mounts a component, it stores the instance in a global variable. When hooks been called inside the setup function, it will use the global variable to get the current component instance.</p>
<pre><code class="language-js">let currentInstance = null

// (pseudo code)
export function mountComponent(component) {
  const instance = createComponent(component)

  // hold the previous instance
  const prev = currentInstance

  // set the instance to global
  currentInstance = instance

  // hooks called inside the `setup()` will have
  // the `currentInstance` as the context
  component.setup() 

  // restore the previous instance
  currentInstance = prev 
}
</code></pre>
<p>A simplified <code>onMounted</code> implementation would be like:</p>
<pre><code class="language-js">// (pseudo code)
export function onMounted(fn) {
  if (!currentInstance) {
    warn(`&quot;onMounted&quot; can't be called outside of component setup()`)
    return
  }

  // bound listener to the current instance
  currentInstance.onMounted(fn)
}
</code></pre>
<p>With this approach, as long as the <code>onMounted</code> is called inside the component <code>setup()</code>, it will be able to get the instance of the current component.</p>
<h2>The Limitation</h2>
<p>So far so good, but what's wrong with asynchronous functions?</p>
<p>The implementation would work based on the fact that JavaScript is <strong>single-threaded</strong>. Single thread makes sure the following statements will be executed right next to each other, which in other words, there is no one could accidentally modify the <code>currentInstance</code> at the same time (a.k.a. it's <a href="https://stackoverflow.com/questions/52196678/what-are-atomic-operations-for-newbies">atomic</a>).</p>
<pre><code class="language-ts">currentInstance = instance
component.setup() 
currentInstance = prev 
</code></pre>
<p>The situation changes when the <code>setup()</code> is asynchronous. Whenever you <code>await</code> a promise, you can think the engine paused the works here and went to do another task. If we <code>await</code> the function, during the time period, multiple components creation will change the global variable unpredictably and end up with a mess.</p>
<pre><code class="language-ts">currentInstance = instance
await component.setup() // atomic lost
currentInstance = prev 
</code></pre>
<p>If we don't use <code>await</code> to check the instance, calling the <code>setup()</code> function will make it finish the tasks before the first <code>await</code> statement, and the rest will be executed whenever the <code>await</code> statement is resolved.</p>
<div class="grid grid-cols-2 gap-2 lt-sm:grid-cols-1">
<pre><code class="language-ts">async function setup() {
  console.log(1)
  await someAsyncFunction()
  console.log(2)
}

console.log(3)
setup()
console.log(4)
</code></pre>
<pre><code class="language-ts">// output:
3
1
4
(awaiting)
2
</code></pre>
</div>
<p>This means, there is no way for Vue to know when will the asynchronous part been called from the outside, so there is also no way to bound the instance to the context.</p>
<h2>The Solutions</h2>
<p>This is actually a limitation of JavaScript itself, unless we have some new proposal to open the gate on the language level, we have to live with it.</p>
<p>But to work around it, I have collected a few solutions for you to choose from based on your needs.</p>
<h3>Remember the Caveat and Avoid It</h3>
<p>This is, of course, an obvious &quot;solution&quot;. You can try to move your effect and hooks before the first <code>await</code> statement and carefully remember not to have them after that again.</p>
<p>Luckily, if you are using ESLint, you can have the <a href="https://eslint.vuejs.org/rules/no-watch-after-await.html"><code>vue/no-watch-after-await</code></a> and <a href="https://eslint.vuejs.org/rules/no-lifecycle-after-await.html"><code>vue/no-lifecycle-after-await</code></a> rules from <a href="https://eslint.vuejs.org/"><code>eslint-plugin-vue</code></a> enabled so it could warn you whenever you made some mistakes (they are enabled by default within the plugin presets).</p>
<h3>Wrap the Async Function as &quot;Reactive Sync&quot;</h3>
<p>In some situations, your logic might be relying on the data that fetched asynchronously. In this way, you could consider using the <a href="/posts/composable-vue-vueday-2021#async-to-sync">trick I have shared on VueDay 2021</a> to <strong>turn your async function into a sync reactive state</strong>.</p>
<pre><code class="language-ts">const data = await fetch('https://api.github.com/').then(r =&gt; r.json())

const user = data.user
</code></pre>
<pre><code class="language-ts">const data = ref(null)

fetch('https://api.github.com/')
  .then(r =&gt; r.json())
  .then(res =&gt; data.value = res)

const user = computed(() =&gt; data?.user)
</code></pre>
<p>This approach make the &quot;connections&quot; between your logic to resolve first, and then reactive updates when the asynchronous function get resolved and filled with data.</p>
<p>There is also some more general utilities for it from <a href="https://vueuse.org/">VueUse</a>:</p>
<h4><a href="https://vueuse.org/useAsyncState"><code>useAsyncState</code></a></h4>
<pre><code class="language-ts">import { useAsyncState } from '@vueuse/core'

const { state, ready } = useAsyncState(async () =&gt; {
  const { data } = await axios.get('https://api.github.com/')
  return { data }
})

const user = computed(() =&gt; state?.user)
</code></pre>
<h4><a href="https://vueuse.org/useFetch"><code>useFetch</code></a></h4>
<pre><code class="language-ts">import { useFetch } from '@vueuse/core'

const { data, isFetching, error } = useFetch('https://api.github.com/')

const user = computed(() =&gt; data?.user)
</code></pre>
<h3>Explicitly Bound the Instance</h3>
<p>Lifecycle hooks actually accept a second argument for setting the instance explicitly.</p>
<pre><code class="language-ts">export default defineAsyncComponent({
  async setup() {
    // get and hold the instance before `await`
    const instance = getCurrentInstance()

    await someAsyncFunction() // &lt;-----------

    onUnmounted(
      () =&gt; console.log('Unmounted'),
      instance // &lt;--- pass the instance to it
    )
  }
})
</code></pre>
<p>However, the downside is that this solution <strong>does not work</strong> with <code>watch</code> / <code>watchEffect</code> / <code>computed</code> / <code>provide</code> / <code>inject</code> as they does not accept the instance argument.</p>
<p>To get the effects work, you could use the <a href="https://github.com/vuejs/rfcs/blob/master/active-rfcs/0041-reactivity-effect-scope.md"><code>effectScope</code> API</a> in the upcoming Vue 3.2.</p>
<pre><code class="language-ts">import { effectScope } from 'vue'

export default defineAsyncComponent({
  async setup() {
    // create the scope before `await`, so it will be bond to the instance
    const scope = effectScope()

    const data = await someAsyncFunction() // &lt;-----------

    scope.run(() =&gt; {
      /* Use `computed`, `watch`, etc. ... */
    })

    // the lifecycle hooks will not be available here,
    // you will need to combine it with the previous snippet
    // to have both lifecycle hooks and effects works.
  }
})
</code></pre>
<h3>Compile-time Magic!</h3>
<p>In the recent <a href="https://github.com/vuejs/rfcs/blob/master/active-rfcs/0040-script-setup.md"><code>&lt;script setup&gt;</code> proposal</a> update, a new <a href="https://github.com/vuejs/rfcs/blob/master/active-rfcs/0040-script-setup.md#top-level-await">compile-time magic</a> is introduced.</p>
<p>The way it works is to inject a script after each <code>await</code> statement for restoring the current instance state.</p>
<pre><code class="language-html">&lt;script setup&gt;
const post = await fetch(`/api/post/1`).then((r) =&gt; r.json())
&lt;/script&gt;
</code></pre>
<pre><code class="language-js">import { withAsyncContext } from 'vue'

export default {
  async setup() {
    let __temp, __restore

    const post =
      (([__temp, __restore] = withAsyncContext(() =&gt;
        fetch(`/api/post/1`).then((r) =&gt; r.json())
      )),
      (__temp = await __temp),
      __restore(),
      __temp)

    // current instance context preserved
    // e.g. onMounted() will still work.

    return { post }
  }
}
</code></pre>
<p>With it, the async functions will <strong>just work</strong> when using with <code>&lt;script setup&gt;</code>. The only shame is it does not work outside of <code>&lt;script setup&gt;</code>.</p>
]]></content:encoded>
            <author>hi@antfu.me (Anthony Fu)</author>
        </item>
        <item>
            <title><![CDATA[About Yak Shaving]]></title>
            <link>https://antfu.me/posts/about-yak-shaving</link>
            <guid>https://antfu.me/posts/about-yak-shaving</guid>
            <pubDate>Wed, 19 May 2021 16:00:00 GMT</pubDate>
            <content:encoded><![CDATA[<p>[[toc]]</p>
<blockquote>
<p><a href="/posts/about-yak-shaving-zh">中文原文 Original in Chinese</a></p>
</blockquote>
<p>I recently visited Zhihu occasionally and saw many questions about how to start open source, or how to make open source projects successful. I kinda had similar doubts for a long time, so I thought that maybe I could share some of my rough views on this.</p>
<p>If you don't know me, I am a team member of Vue, Vite, wenyan-lang, WindiCSS, Intlify, and the author of VueUse, Slidev, Type Challenges and i18n Ally. I also have some small open source tools under my personal GitHub account, you can have a look at my <a href="https://antfu.me/projects">full project list</a>. Since I started doing open source in earnest almost two years ago, even though those contributions aren't that impressive, they still managed to allow me to <a href="https://twitter.com/antfu7/status/1362676666221268995">work full-time on open source development and maintenance through sponsorships</a>.</p>
<p>Many people probably gonna tell you that the success of a project depends on opportunity, marketing, branding, or documentation, ecosystem, technical innovation, code quality, etc. All of these are indeed important, but for me, the most important thing is the motivation to start a project and the drive to do it well. For me, the best way to get the power is via Yak Shaving.</p>
<h2>Yak Shaving</h2>
<p><a href="https://americanexpress.io/yak-shaving">Yak Shaving</a> refers to a series of actions when you're working on one task and then you find another task that's not finished, you tackle that one first, and while you're working on that one, you find another task to do... and so forth, so that you stray from the work that should have been done, and end up not getting nothing finished. Here is a real-world example:</p>
<blockquote>
<p>You want to bake an apple pie, so you head to the kitchen.<br><br>
In the hallway, you notice some paint chipping on the wall.<br><br>
So you walk to the hardware store for some paint.<br><br>
On the way, you pass a bakery and stop in for a cupcake.<br><br>
While eating the cupcake, you feel a pain in your mouth. It’s that cavity that you’ve been putting off.<br><br>
You pick up your phone to call the dentist to make an appointment, but you see a notification from your friend Cher, who’s having a party.<br><br>
You don’t want to show up empty-handed, so you stop for a bottle of wine…</p>
</blockquote>
<p>An example that more relevant to developers might be: You planned to write a blog today, but you found out none of the existing tools are good enough for you. Then you spend a month writing your own static website generator, but end up with the generator unfinished and forgetting about writing the blog.</p>
<p>I guess we all had similar experiences more or less. Yak Shaving usually refers to something negative, emphasizing inattentiveness or lack of clarity of purpose. But I kinda think it's also an important source of motivation for many things. When a person needs a tool, they are most motivated to solve it and make it happen. I, not coincidentally, am an obsessive Yak Shaving fan.</p>
<p>Maybe I'll share some of my stories with you to give you a better idea of what I'm trying to express:</p>
<h2>The story of how I started doing open source</h2>
<p>In my senior year of college, I went on a graduation trip to the Philippines with a group of college friends. Because of the various problems of exchanging foreign currency, an operation down to Taiwan dollars, US dollars, Philippine pesos and different exchange rates each time, making the record of public accounts and settlement very complicated. After we came back from the trip, we came up with the idea of making an app to solve this problem.</p>
<p>To make the app reach a large enough audience, multilingual internationalization is something we have to consider. As there are many foreign language departments in our college, we thought we could use our resources to get our friends to help on translating the App into multiple languages. However, it is obviously unrealistic to have foreign language students write JSON with bare hands, so we had to find something a little easier. Luckily, I found <a href="https://github.com/think2011/vscode-vue-i18n"><code>think2011/vscode-vue-i18n</code></a>, which looks great, but lacks some features we needed. So I contacted the author and got fork permission, and then, here comes the <a href="https://github.com/lokalise/i18n-ally">i18n Ally</a> project.</p>
<p>The later stage of App development coincided with the Composition API RFC of Vue 3. The new API seemed to solve many of the pain points in our development. In the spirit of experimentation, we installed the Vue 2 plugin and started to try it out. In the process of using it, we found out there are quite some functions we are commonly used, and also inspired by <a href="https://github.com/streamich/react-use"><code>react-use</code></a>, I extracted them out and made <a href="https://github.com/vueuse/vueuse">VueUse</a>.</p>
<p>Given that Vue 3 was still in Alpha at the time, and the community needs to gradually migrate from Vue 2 to Vue 3 for a long time in the future. I made VueUse intentionally as a universal library for Vue 2 and Vue 3 so that people could migrate seamlessly. The initial solution was to publish two packages for Vue 2 and 3 under different npm tags. As Vue 3 matured, more and more libraries wanted to go the same way to reduce the cost of maintaining two codebases at the same time. Then I thought, maybe I can find a general solution from VueUse, so that everyone could get benefit from it. And then, <a href="https://github.com/vueuse/vue-demi"><code>vue-demi</code></a> comes out. As a result, it also allows VueUse to publish one version that supports both Vue 2 and 3 at the same time.</p>
<p>VueUse's support for Vue 2 relies on the <a href="https://github.com/vuejs/composition-api"><code>@vue/composition-api</code></a> library, at some point, there are some inconsistencies in the plugin with the latest Vue 3 changes. Which results in VueUse's development being hampered. After a quite long time of no response to the PR from the repository, I thought I might be able to help out a bit, so I posted <a href="https://github.com/vuejs/composition-api/issues/343">an issue</a> in the repository saying I would like to volunteer myself maintaining the project. And that's also the opportunity for me to join the Vue team.</p>
<p>In the end, our App didn't work out, but I gained a lot of valuable experience solving problems and working on open source projects along the way. i18n Ally started out as a vue-i18n specific extension and now supports over 20 major frameworks, with over 60,000 downloads of VS Code. VueUse started as a simple toolset and now it becomes a GitHub Organization with 10 members and 8 ecosystem packages.</p>
<p>I could probably tell stories like these for a whole day, and behind almost every project there is such a motivation to try to solve a certain problem. After all of this harangue, the point I'm trying to make is that Yak Shaving can be a great engine for progressing when used properly.</p>
<p>And here are my methods of how to make Yak Shaving a good thing:</p>
<h2>Shave the Good Yak</h2>
<h3>Identify Problems</h3>
<p>I spent every day of my four years in college thinking if I could make an interesting open source project that everyone needed and live as a full-time open sourceror with freedom. The difference is that the four years I was thinking about how to make something that other people wanted, but later I was solving <strong>the actual problems that I encountered</strong>. As said, when you need a tool, you have the most motivation to make it. And also as a user, you know the best where the pain points and needs are. When you encounter this problem, others might have just encountered a similar one.</p>
<h3>Solve the Problem</h3>
<p>The most basic rule to start trying to solve a problem is to look for existing solutions, if the problem has been well solved, which meets your all needs, then just use it. Reinventing wheels might be a good way to learn, but since the wheels are already there, there has to be someone thinking about how to build a car, right?</p>
<p>When you find that there is no solution to the problem, or that the existing solutions don't work for you, while you have a great idea in your mind. Congratulations, you've found a great Yak.</p>
<h3>Good Enough</h3>
<p>The most important part of making Yak Shaving great is to <strong>just be good enough</strong> -- do not have too much expectation, if the idea is verified feasible, make it just good enough; if the idea does now work, don't be discouraged, just throw it away, maybe one day you can pick it back up again with new ideas. It's not necessary to be perfect at the beginning. You don't have to draw a grand blueprint or plan, you don't have to set a huge goal of how many stars or how many users - you are doing it for yourself, and make it just good enough to solve your own problems. The important thing is to not spending too much time on a single idea, and get back to what you should have been doing in time.</p>
<h3>Refine the Project</h3>
<p>As the first user of your own product, you will find a lot of things for improvement in the process of using it, go and modify it from time to time and do some improvements. If you got more time, you can add a README describing the problems you encountered and the motivation for doing the project, which may be helpful to someone who faced similar problems.</p>
<p>In the end, when using the project makes you feel that it's a pretty good idea, while you have completed the work that should have been done, you may wish to spend some time writing a document, improve the implementations, and promote a little. It's best if it has been recognized, but if not, just treat it as an exercise, and at least you've your own problems solved. If the responses went well, then someone will start to raise issues and send PRs. Which more enhancements and features coming, you will also gradually find the future direction of the project. Besides that, those changes and enhancements from the community may end up being a better solution to the problems you encountered at the beginning.</p>
<h3>Identify More Problems</h3>
<p>The way to find more problems is simple, learn more and try more. In the process of solving and improving the problem, you will likely find new problems that could potentially be solved. Issues from the community can also help you find more inspiration. Anyway, congratulations on entering the positive cycle!</p>
<h2>Wrapping Up</h2>
<p>Hopefully, this insight could give you some inspiration on solving your own problems, or making a good open source product, in one way or another.</p>
<p>I also recommend some awesome Yak Shaving masters, maybe their projects and experiences can give you some inspiration as well:</p>
<ul>
<li><a href="https://github.com/sindresorhus">Sindre Sorhus</a> - actively maintains 1100+ npm packages, Webpack and Babel both rely on 100+ of his packages</li>
<li><a href="https://github.com/tj">TJ Holowaychuk</a> - Author of koa, mocha, express, etc.</li>
<li><a href="https://github.com/lukeed">Luke Edwards</a> - Author of polka, uvu, klona, etc.</li>
<li><a href="https://github.com/egoist">Egoist</a> - Author of poi, cac, saber, etc.</li>
<li><a href="https://github.com/privatenumber">Hiroki Osame</a> - Author of esbuild-loader, vue-2-3, etc.</li>
</ul>
<p>Cheers, and happy hacking!</p>
]]></content:encoded>
            <author>hi@antfu.me (Anthony Fu)</author>
        </item>
    </channel>
</rss>